{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Hyperparameter Tuning Per Reward Function\n",
    "\n",
    "This notebook tunes DQN hyperparameters separately for each reward function.\n",
    "Outputs are saved under `results/hyperparameter_tuning/dqn_per_reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reinforcement_learning_taxi.agents.dqn_agent import DQNAgent\n",
    "from reinforcement_learning_taxi.environments import make_taxi_env\n",
    "from reinforcement_learning_taxi.environments.reward_wrappers import REWARD_FUNCTIONS\n",
    "from reinforcement_learning_taxi.evaluation.metrics import evaluate_agent\n",
    "from reinforcement_learning_taxi.training.dqn_trainer import DQNTrainer\n",
    "from reinforcement_learning_taxi.utils.config_utils import load_config, save_optimized_config\n",
    "from reinforcement_learning_taxi.utils.path_utils import get_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward functions: ['default', 'distance_based', 'modified_penalty', 'enhanced']\n",
      "Training timesteps: 250,000\n",
      "Random configs per reward: 5\n",
      "Evaluation episodes: 100\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = get_repo_root()\n",
    "RESULTS_DIR = ROOT_DIR / 'results/hyperparameter_tuning/dqn_per_reward'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DQN_BASE_CONFIG = load_config(ROOT_DIR / 'src/reinforcement_learning_taxi/configs/dqn_config_baseline.yaml')\n",
    "DQN_BASE_AGENT = DQN_BASE_CONFIG['agent']\n",
    "\n",
    "TRAINING_TIMESTEPS = 250000\n",
    "N_EVAL_EPISODES = 100\n",
    "N_RANDOM_CONFIGS = 5\n",
    "SEED = 42\n",
    "\n",
    "reward_functions = list(REWARD_FUNCTIONS.keys())\n",
    "print(f'Reward functions: {reward_functions}')\n",
    "print(f'Training timesteps: {TRAINING_TIMESTEPS:,}')\n",
    "print(f'Random configs per reward: {N_RANDOM_CONFIGS}')\n",
    "print(f'Evaluation episodes: {N_EVAL_EPISODES}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Search Space:\n",
      "  learning_rate: [5e-05, 0.0001, 0.0002, 0.0005]\n",
      "  batch_size: [32, 64, 128]\n",
      "  buffer_size: [50000, 100000, 200000]\n",
      "  gamma: [0.95, 0.99, 0.999]\n",
      "  target_update_interval: [500, 1000, 2000]\n",
      "  exploration_fraction: [0.8, 0.9]\n",
      "  exploration_final_eps: [0.1, 0.2, 0.3]\n",
      "  learning_starts: [10000, 20000]\n",
      "  train_freq: [1, 4]\n",
      "  gradient_steps: [1, 4]\n",
      "Total possible combinations: 15552\n"
     ]
    }
   ],
   "source": [
    "dqn_param_space = {\n",
    "    'learning_rate': [5e-5, 1e-4, 2e-4, 5e-4],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'buffer_size': [50000, 100000, 200000],\n",
    "    'gamma': [0.95, 0.99, 0.999],\n",
    "    'target_update_interval': [500, 1000, 2000],\n",
    "    'exploration_fraction': [0.8, 0.9],\n",
    "    'exploration_final_eps': [0.1, 0.2, 0.3],\n",
    "    'learning_starts': [10000, 20000],\n",
    "    'train_freq': [1, 4],\n",
    "    'gradient_steps': [1, 4],\n",
    "}\n",
    "\n",
    "print('DQN Search Space:')\n",
    "for param, values in dqn_param_space.items():\n",
    "    print(f'  {param}: {values}')\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in dqn_param_space.values()])\n",
    "print(f'Total possible combinations: {total_combinations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_params(param_space, n_samples, seed=42):\n",
    "    random.seed(seed)\n",
    "    configs = []\n",
    "    for _ in range(n_samples):\n",
    "        config = {param: random.choice(values) for param, values in param_space.items()}\n",
    "        configs.append(config)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DQN tuning for reward: default\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/5: {'learning_rate': 5e-05, 'batch_size': 32, 'buffer_size': 200000, 'gamma': 0.99, 'target_update_interval': 500, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.1, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-147.61 +/- 90.75\n",
      "Episode length: 152.86 +/- 81.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-98.02 +/- 104.06\n",
      "Episode length: 108.31 +/- 93.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=8.07 +/- 2.64\n",
      "Episode length: 12.93 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=8.37 +/- 2.77\n",
      "Episode length: 12.63 +/- 2.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.02 +/- 2.60\n",
      "Episode length: 12.98 +/- 2.60\n",
      "Eval num_timesteps=140000, episode_reward=7.88 +/- 2.53\n",
      "Episode length: 13.12 +/- 2.53\n",
      "Eval num_timesteps=160000, episode_reward=8.22 +/- 2.29\n",
      "Episode length: 12.78 +/- 2.29\n",
      "Eval num_timesteps=180000, episode_reward=8.08 +/- 2.62\n",
      "Episode length: 12.92 +/- 2.62\n",
      "Eval num_timesteps=200000, episode_reward=8.21 +/- 2.72\n",
      "Episode length: 12.79 +/- 2.72\n",
      "Eval num_timesteps=220000, episode_reward=8.05 +/- 2.70\n",
      "Episode length: 12.95 +/- 2.70\n",
      "Eval num_timesteps=240000, episode_reward=7.81 +/- 2.61\n",
      "Episode length: 13.19 +/- 2.61\n",
      "  Mean reward (shaped): 8.28\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 2/5: {'learning_rate': 5e-05, 'batch_size': 32, 'buffer_size': 50000, 'gamma': 0.95, 'target_update_interval': 500, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.3, 'learning_starts': 10000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-191.48 +/- 41.74\n",
      "Episode length: 192.32 +/- 37.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=-183.07 +/- 57.42\n",
      "Episode length: 184.75 +/- 51.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-170.72 +/- 72.58\n",
      "Episode length: 173.66 +/- 65.29\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -174.90\n",
      "  Success rate (default): 16.00%\n",
      "\n",
      "Configuration 3/5: {'learning_rate': 0.0005, 'batch_size': 128, 'buffer_size': 100000, 'gamma': 0.95, 'target_update_interval': 500, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.2, 'learning_starts': 20000, 'train_freq': 1, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-1964.00 +/- 252.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-112.47 +/- 102.88\n",
      "Episode length: 121.29 +/- 92.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=7.92 +/- 2.53\n",
      "Episode length: 13.08 +/- 2.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=7.51 +/- 2.58\n",
      "Episode length: 13.49 +/- 2.58\n",
      "Eval num_timesteps=100000, episode_reward=7.97 +/- 2.77\n",
      "Episode length: 13.03 +/- 2.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=7.60 +/- 2.65\n",
      "Episode length: 13.40 +/- 2.65\n",
      "Eval num_timesteps=140000, episode_reward=8.16 +/- 2.80\n",
      "Episode length: 12.84 +/- 2.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=7.50 +/- 2.85\n",
      "Episode length: 13.50 +/- 2.85\n",
      "Eval num_timesteps=180000, episode_reward=7.65 +/- 2.46\n",
      "Episode length: 13.35 +/- 2.46\n",
      "Eval num_timesteps=200000, episode_reward=7.88 +/- 2.52\n",
      "Episode length: 13.12 +/- 2.52\n",
      "Eval num_timesteps=220000, episode_reward=8.11 +/- 2.50\n",
      "Episode length: 12.89 +/- 2.50\n",
      "Eval num_timesteps=240000, episode_reward=7.66 +/- 2.48\n",
      "Episode length: 13.34 +/- 2.48\n",
      "  Mean reward (shaped): 7.86\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 4/5: {'learning_rate': 0.0002, 'batch_size': 32, 'buffer_size': 50000, 'gamma': 0.99, 'target_update_interval': 500, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.2, 'learning_starts': 20000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-1964.00 +/- 252.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=7.89 +/- 2.30\n",
      "Episode length: 13.11 +/- 2.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=7.70 +/- 2.66\n",
      "Episode length: 13.30 +/- 2.66\n",
      "Eval num_timesteps=100000, episode_reward=7.53 +/- 2.22\n",
      "Episode length: 13.47 +/- 2.22\n",
      "Eval num_timesteps=120000, episode_reward=8.04 +/- 2.46\n",
      "Episode length: 12.96 +/- 2.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=7.88 +/- 2.58\n",
      "Episode length: 13.12 +/- 2.58\n",
      "Eval num_timesteps=160000, episode_reward=8.07 +/- 2.64\n",
      "Episode length: 12.93 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=7.94 +/- 2.84\n",
      "Episode length: 13.06 +/- 2.84\n",
      "Eval num_timesteps=200000, episode_reward=7.82 +/- 2.78\n",
      "Episode length: 13.18 +/- 2.78\n",
      "Eval num_timesteps=220000, episode_reward=8.27 +/- 2.60\n",
      "Episode length: 12.73 +/- 2.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=7.90 +/- 2.71\n",
      "Episode length: 13.10 +/- 2.71\n",
      "  Mean reward (shaped): 7.75\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 5/5: {'learning_rate': 5e-05, 'batch_size': 64, 'buffer_size': 50000, 'gamma': 0.999, 'target_update_interval': 1000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.3, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-157.83 +/- 84.34\n",
      "Episode length: 162.03 +/- 75.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-156.22 +/- 84.92\n",
      "Episode length: 160.63 +/- 76.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-139.69 +/- 94.37\n",
      "Episode length: 145.78 +/- 84.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-16.46 +/- 67.82\n",
      "Episode length: 34.94 +/- 61.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-102.45 +/- 103.60\n",
      "Episode length: 112.32 +/- 93.13\n",
      "Eval num_timesteps=160000, episode_reward=-135.22 +/- 96.66\n",
      "Episode length: 141.73 +/- 86.95\n",
      "Eval num_timesteps=180000, episode_reward=7.89 +/- 2.64\n",
      "Episode length: 13.11 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=8.51 +/- 2.94\n",
      "Episode length: 12.49 +/- 2.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=8.41 +/- 2.43\n",
      "Episode length: 12.59 +/- 2.43\n",
      "Eval num_timesteps=240000, episode_reward=7.77 +/- 2.79\n",
      "Episode length: 13.23 +/- 2.79\n",
      "  Mean reward (shaped): 7.81\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Saved DQN tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_tuning_default.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_default_best.yaml\n",
      "\n",
      "======================================================================\n",
      "DQN tuning for reward: distance_based\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/5: {'learning_rate': 0.0005, 'batch_size': 32, 'buffer_size': 100000, 'gamma': 0.99, 'target_update_interval': 1000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.2, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-117.60 +/- 102.88\n",
      "Episode length: 126.21 +/- 92.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-5.73 +/- 53.35\n",
      "Episode length: 26.11 +/- 47.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=9.03 +/- 2.42\n",
      "Episode length: 12.88 +/- 2.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=8.47 +/- 2.54\n",
      "Episode length: 13.41 +/- 2.67\n",
      "Eval num_timesteps=100000, episode_reward=8.70 +/- 2.38\n",
      "Episode length: 13.24 +/- 2.53\n",
      "Eval num_timesteps=120000, episode_reward=8.77 +/- 2.40\n",
      "Episode length: 13.17 +/- 2.58\n",
      "Eval num_timesteps=140000, episode_reward=8.78 +/- 2.51\n",
      "Episode length: 13.11 +/- 2.67\n",
      "Eval num_timesteps=160000, episode_reward=8.81 +/- 2.31\n",
      "Episode length: 13.09 +/- 2.48\n",
      "Eval num_timesteps=180000, episode_reward=8.92 +/- 2.31\n",
      "Episode length: 12.98 +/- 2.45\n",
      "Eval num_timesteps=200000, episode_reward=8.85 +/- 2.76\n",
      "Episode length: 13.03 +/- 2.92\n",
      "Eval num_timesteps=220000, episode_reward=8.66 +/- 2.65\n",
      "Episode length: 13.28 +/- 2.85\n",
      "Eval num_timesteps=240000, episode_reward=8.81 +/- 2.52\n",
      "Episode length: 13.10 +/- 2.69\n",
      "  Mean reward (shaped): 9.03\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 2/5: {'learning_rate': 0.0001, 'batch_size': 128, 'buffer_size': 200000, 'gamma': 0.95, 'target_update_interval': 2000, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.2, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-193.41 +/- 35.88\n",
      "Episode length: 194.34 +/- 32.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-32.72 +/- 83.65\n",
      "Episode length: 50.30 +/- 74.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=8.91 +/- 2.53\n",
      "Episode length: 13.00 +/- 2.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=8.78 +/- 2.60\n",
      "Episode length: 13.10 +/- 2.79\n",
      "Eval num_timesteps=100000, episode_reward=9.45 +/- 2.37\n",
      "Episode length: 12.39 +/- 2.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.79 +/- 2.20\n",
      "Episode length: 13.09 +/- 2.33\n",
      "Eval num_timesteps=140000, episode_reward=8.45 +/- 2.44\n",
      "Episode length: 13.49 +/- 2.57\n",
      "Eval num_timesteps=160000, episode_reward=8.89 +/- 2.60\n",
      "Episode length: 13.03 +/- 2.79\n",
      "Eval num_timesteps=180000, episode_reward=8.77 +/- 2.51\n",
      "Episode length: 13.14 +/- 2.65\n",
      "Eval num_timesteps=200000, episode_reward=9.22 +/- 2.44\n",
      "Episode length: 12.68 +/- 2.63\n",
      "Eval num_timesteps=220000, episode_reward=8.92 +/- 2.37\n",
      "Episode length: 12.96 +/- 2.53\n",
      "Eval num_timesteps=240000, episode_reward=8.53 +/- 2.78\n",
      "Episode length: 13.39 +/- 2.98\n",
      "  Mean reward (shaped): 8.89\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 3/5: {'learning_rate': 0.0002, 'batch_size': 32, 'buffer_size': 50000, 'gamma': 0.99, 'target_update_interval': 2000, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.2, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-180.85 +/- 60.30\n",
      "Episode length: 182.99 +/- 54.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-134.77 +/- 96.94\n",
      "Episode length: 141.71 +/- 86.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-3.52 +/- 49.69\n",
      "Episode length: 24.11 +/- 44.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=9.13 +/- 2.57\n",
      "Episode length: 12.74 +/- 2.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=8.60 +/- 2.18\n",
      "Episode length: 13.36 +/- 2.36\n",
      "Eval num_timesteps=120000, episode_reward=8.59 +/- 2.22\n",
      "Episode length: 13.33 +/- 2.32\n",
      "Eval num_timesteps=140000, episode_reward=8.86 +/- 2.33\n",
      "Episode length: 13.01 +/- 2.44\n",
      "Eval num_timesteps=160000, episode_reward=8.45 +/- 2.34\n",
      "Episode length: 13.50 +/- 2.51\n",
      "Eval num_timesteps=180000, episode_reward=8.84 +/- 2.31\n",
      "Episode length: 13.10 +/- 2.44\n",
      "Eval num_timesteps=200000, episode_reward=8.68 +/- 2.41\n",
      "Episode length: 13.23 +/- 2.58\n",
      "Eval num_timesteps=220000, episode_reward=8.80 +/- 2.29\n",
      "Episode length: 13.07 +/- 2.44\n",
      "Eval num_timesteps=240000, episode_reward=8.45 +/- 2.53\n",
      "Episode length: 13.52 +/- 2.71\n",
      "  Mean reward (shaped): 8.94\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 4/5: {'learning_rate': 0.0005, 'batch_size': 32, 'buffer_size': 200000, 'gamma': 0.99, 'target_update_interval': 1000, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.2, 'learning_starts': 20000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-1946.00 +/- 307.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.82 +/- 0.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-199.81 +/- 0.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-168.31 +/- 75.13\n",
      "Episode length: 171.69 +/- 67.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-101.47 +/- 104.60\n",
      "Episode length: 111.76 +/- 93.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-135.00 +/- 96.34\n",
      "Episode length: 142.09 +/- 86.41\n",
      "Eval num_timesteps=140000, episode_reward=-38.95 +/- 88.00\n",
      "Episode length: 55.83 +/- 78.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.63 +/- 2.41\n",
      "Episode length: 13.33 +/- 2.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=8.86 +/- 2.33\n",
      "Episode length: 13.03 +/- 2.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=8.52 +/- 2.44\n",
      "Episode length: 13.40 +/- 2.59\n",
      "Eval num_timesteps=220000, episode_reward=8.57 +/- 2.51\n",
      "Episode length: 13.35 +/- 2.67\n",
      "Eval num_timesteps=240000, episode_reward=9.07 +/- 2.50\n",
      "Episode length: 12.80 +/- 2.64\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 9.05\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 5/5: {'learning_rate': 5e-05, 'batch_size': 128, 'buffer_size': 50000, 'gamma': 0.999, 'target_update_interval': 500, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.3, 'learning_starts': 10000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-199.81 +/- 0.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.79 +/- 0.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-199.86 +/- 0.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-199.83 +/- 0.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-199.86 +/- 0.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-199.87 +/- 0.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-199.83 +/- 0.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-199.90 +/- 0.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-199.78 +/- 0.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-199.82 +/- 0.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-199.80 +/- 0.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-195.63 +/- 29.32\n",
      "Episode length: 196.24 +/- 26.32\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -191.41\n",
      "  Success rate (default): 1.00%\n",
      "\n",
      "Saved DQN tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_tuning_distance_based.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_distance_based_best.yaml\n",
      "\n",
      "======================================================================\n",
      "DQN tuning for reward: modified_penalty\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/5: {'learning_rate': 0.0002, 'batch_size': 32, 'buffer_size': 100000, 'gamma': 0.999, 'target_update_interval': 2000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.3, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-89.64 +/- 32.94\n",
      "Episode length: 182.97 +/- 54.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-91.92 +/- 29.43\n",
      "Episode length: 186.72 +/- 48.41\n",
      "Eval num_timesteps=80000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-81.51 +/- 42.37\n",
      "Episode length: 169.58 +/- 69.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-34.81 +/- 56.63\n",
      "Episode length: 92.99 +/- 92.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=4.91 +/- 30.96\n",
      "Episode length: 27.91 +/- 50.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=14.04 +/- 1.11\n",
      "Episode length: 12.91 +/- 2.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=14.10 +/- 1.50\n",
      "Episode length: 12.80 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=13.89 +/- 1.36\n",
      "Episode length: 13.22 +/- 2.72\n",
      "Eval num_timesteps=220000, episode_reward=14.05 +/- 1.32\n",
      "Episode length: 12.89 +/- 2.64\n",
      "Eval num_timesteps=240000, episode_reward=13.88 +/- 1.11\n",
      "Episode length: 13.25 +/- 2.22\n",
      "  Mean reward (shaped): 14.11\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 2/5: {'learning_rate': 0.0002, 'batch_size': 128, 'buffer_size': 50000, 'gamma': 0.95, 'target_update_interval': 2000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.1, 'learning_starts': 20000, 'train_freq': 4, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-1943.00 +/- 324.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-96.53 +/- 19.76\n",
      "Episode length: 194.28 +/- 32.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-58.51 +/- 55.32\n",
      "Episode length: 131.78 +/- 90.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=6.17 +/- 29.15\n",
      "Episode length: 25.80 +/- 47.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=13.89 +/- 1.30\n",
      "Episode length: 13.21 +/- 2.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=14.15 +/- 1.32\n",
      "Episode length: 12.69 +/- 2.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=14.11 +/- 1.30\n",
      "Episode length: 12.78 +/- 2.61\n",
      "Eval num_timesteps=160000, episode_reward=13.98 +/- 1.29\n",
      "Episode length: 13.04 +/- 2.58\n",
      "Eval num_timesteps=180000, episode_reward=13.97 +/- 1.36\n",
      "Episode length: 13.06 +/- 2.72\n",
      "Eval num_timesteps=200000, episode_reward=13.99 +/- 1.33\n",
      "Episode length: 13.01 +/- 2.66\n",
      "Eval num_timesteps=220000, episode_reward=13.85 +/- 1.24\n",
      "Episode length: 13.31 +/- 2.48\n",
      "Eval num_timesteps=240000, episode_reward=13.87 +/- 1.16\n",
      "Episode length: 13.26 +/- 2.31\n",
      "  Mean reward (shaped): 14.13\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 3/5: {'learning_rate': 0.0001, 'batch_size': 32, 'buffer_size': 50000, 'gamma': 0.999, 'target_update_interval': 1000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.2, 'learning_starts': 20000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-1981.00 +/- 189.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-95.39 +/- 22.58\n",
      "Episode length: 192.42 +/- 37.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-95.38 +/- 22.63\n",
      "Episode length: 192.40 +/- 37.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-81.66 +/- 42.02\n",
      "Episode length: 169.88 +/- 69.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-81.75 +/- 41.81\n",
      "Episode length: 170.07 +/- 68.59\n",
      "Eval num_timesteps=220000, episode_reward=-70.40 +/- 49.94\n",
      "Episode length: 151.46 +/- 81.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-54.37 +/- 55.90\n",
      "Episode length: 125.13 +/- 91.71\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -110.95\n",
      "  Success rate (default): 9.00%\n",
      "\n",
      "Configuration 4/5: {'learning_rate': 0.0001, 'batch_size': 128, 'buffer_size': 50000, 'gamma': 0.95, 'target_update_interval': 500, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.1, 'learning_starts': 20000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-1981.00 +/- 189.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.03 +/- 32.73\n",
      "Episode length: 29.25 +/- 53.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=13.88 +/- 1.32\n",
      "Episode length: 13.25 +/- 2.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=14.00 +/- 1.27\n",
      "Episode length: 13.00 +/- 2.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=14.00 +/- 1.31\n",
      "Episode length: 13.00 +/- 2.62\n",
      "Eval num_timesteps=120000, episode_reward=14.06 +/- 1.25\n",
      "Episode length: 12.88 +/- 2.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=14.13 +/- 1.38\n",
      "Episode length: 12.73 +/- 2.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=13.88 +/- 1.34\n",
      "Episode length: 13.24 +/- 2.69\n",
      "Eval num_timesteps=180000, episode_reward=13.84 +/- 1.35\n",
      "Episode length: 13.32 +/- 2.69\n",
      "Eval num_timesteps=200000, episode_reward=13.91 +/- 1.26\n",
      "Episode length: 13.17 +/- 2.51\n",
      "Eval num_timesteps=220000, episode_reward=14.16 +/- 1.27\n",
      "Episode length: 12.68 +/- 2.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=13.89 +/- 1.40\n",
      "Episode length: 13.21 +/- 2.80\n",
      "  Mean reward (shaped): 13.98\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 5/5: {'learning_rate': 0.0002, 'batch_size': 128, 'buffer_size': 50000, 'gamma': 0.95, 'target_update_interval': 2000, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.1, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-75.72 +/- 47.10\n",
      "Episode length: 160.05 +/- 77.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-82.77 +/- 41.02\n",
      "Episode length: 171.69 +/- 67.40\n",
      "Eval num_timesteps=80000, episode_reward=0.17 +/- 37.01\n",
      "Episode length: 35.73 +/- 60.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=14.01 +/- 1.14\n",
      "Episode length: 12.99 +/- 2.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=13.88 +/- 1.22\n",
      "Episode length: 13.25 +/- 2.43\n",
      "Eval num_timesteps=140000, episode_reward=13.82 +/- 1.26\n",
      "Episode length: 13.36 +/- 2.51\n",
      "Eval num_timesteps=160000, episode_reward=13.91 +/- 1.35\n",
      "Episode length: 13.17 +/- 2.69\n",
      "Eval num_timesteps=180000, episode_reward=14.03 +/- 1.22\n",
      "Episode length: 12.94 +/- 2.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=14.12 +/- 1.37\n",
      "Episode length: 12.76 +/- 2.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=13.90 +/- 1.29\n",
      "Episode length: 13.20 +/- 2.58\n",
      "Eval num_timesteps=240000, episode_reward=13.84 +/- 1.36\n",
      "Episode length: 13.32 +/- 2.71\n",
      "  Mean reward (shaped): 14.21\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Saved DQN tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_tuning_modified_penalty.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_modified_penalty_best.yaml\n",
      "\n",
      "======================================================================\n",
      "DQN tuning for reward: enhanced\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/5: {'learning_rate': 5e-05, 'batch_size': 32, 'buffer_size': 200000, 'gamma': 0.95, 'target_update_interval': 500, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.2, 'learning_starts': 10000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-199.58 +/- 0.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.49 +/- 0.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-199.42 +/- 0.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-160.29 +/- 78.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-132.36 +/- 94.12\n",
      "Episode length: 198.08 +/- 19.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-54.39 +/- 86.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-32.46 +/- 70.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-11.05 +/- 38.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-3.09 +/- 1.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-3.06 +/- 2.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=-3.40 +/- 1.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-1.36 +/- 6.56\n",
      "Episode length: 182.99 +/- 54.09\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 1.50\n",
      "  Success rate (default): 13.00%\n",
      "\n",
      "Configuration 2/5: {'learning_rate': 0.0002, 'batch_size': 64, 'buffer_size': 50000, 'gamma': 0.999, 'target_update_interval': 500, 'exploration_fraction': 0.8, 'exploration_final_eps': 0.3, 'learning_starts': 10000, 'train_freq': 1, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-140.89 +/- 87.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-3.46 +/- 1.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=5.33 +/- 10.88\n",
      "Episode length: 126.26 +/- 92.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=4.30 +/- 9.99\n",
      "Episode length: 128.93 +/- 90.79\n",
      "Eval num_timesteps=100000, episode_reward=8.62 +/- 10.34\n",
      "Episode length: 93.00 +/- 92.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=12.04 +/- 8.60\n",
      "Episode length: 52.62 +/- 76.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=7.14 +/- 10.08\n",
      "Episode length: 104.54 +/- 93.59\n",
      "Eval num_timesteps=160000, episode_reward=13.04 +/- 8.46\n",
      "Episode length: 50.15 +/- 74.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=10.45 +/- 9.83\n",
      "Episode length: 74.41 +/- 88.16\n",
      "Eval num_timesteps=200000, episode_reward=5.32 +/- 10.28\n",
      "Episode length: 123.08 +/- 92.29\n",
      "Eval num_timesteps=220000, episode_reward=16.62 +/- 2.50\n",
      "Episode length: 13.18 +/- 2.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=16.39 +/- 2.31\n",
      "Episode length: 13.46 +/- 2.65\n",
      "  Mean reward (shaped): 3.29\n",
      "  Success rate (default): 32.00%\n",
      "\n",
      "Configuration 3/5: {'learning_rate': 0.0002, 'batch_size': 128, 'buffer_size': 200000, 'gamma': 0.999, 'target_update_interval': 2000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.1, 'learning_starts': 10000, 'train_freq': 4, 'gradient_steps': 1}\n",
      "Eval num_timesteps=20000, episode_reward=-199.49 +/- 0.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-14.89 +/- 46.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.08 +/- 7.87\n",
      "Episode length: 171.76 +/- 67.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-59.85 +/- 87.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-86.55 +/- 98.75\n",
      "Episode length: 181.05 +/- 56.85\n",
      "Eval num_timesteps=120000, episode_reward=-94.53 +/- 96.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-43.07 +/- 81.06\n",
      "Episode length: 190.47 +/- 41.54\n",
      "Eval num_timesteps=160000, episode_reward=0.57 +/- 8.53\n",
      "Episode length: 164.29 +/- 73.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=4.20 +/- 10.21\n",
      "Episode length: 132.39 +/- 90.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=2.37 +/- 9.26\n",
      "Episode length: 145.89 +/- 84.68\n",
      "Eval num_timesteps=220000, episode_reward=-1.00 +/- 6.99\n",
      "Episode length: 179.29 +/- 58.91\n",
      "Eval num_timesteps=240000, episode_reward=-1.92 +/- 5.52\n",
      "Episode length: 186.89 +/- 47.79\n",
      "  Mean reward (shaped): 0.11\n",
      "  Success rate (default): 15.00%\n",
      "\n",
      "Configuration 4/5: {'learning_rate': 0.0002, 'batch_size': 128, 'buffer_size': 100000, 'gamma': 0.95, 'target_update_interval': 1000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.1, 'learning_starts': 20000, 'train_freq': 4, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-992.00 +/- 79.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-20.31 +/- 59.13\n",
      "Episode length: 181.11 +/- 56.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=13.17 +/- 8.23\n",
      "Episode length: 46.60 +/- 71.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=16.18 +/- 2.48\n",
      "Episode length: 13.60 +/- 2.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=16.60 +/- 2.35\n",
      "Episode length: 13.23 +/- 2.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=16.88 +/- 2.43\n",
      "Episode length: 12.83 +/- 2.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=16.56 +/- 2.08\n",
      "Episode length: 13.26 +/- 2.34\n",
      "Eval num_timesteps=160000, episode_reward=16.81 +/- 2.19\n",
      "Episode length: 13.01 +/- 2.50\n",
      "Eval num_timesteps=180000, episode_reward=16.84 +/- 2.20\n",
      "Episode length: 12.98 +/- 2.56\n",
      "Eval num_timesteps=200000, episode_reward=16.89 +/- 2.16\n",
      "Episode length: 12.93 +/- 2.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=16.83 +/- 2.30\n",
      "Episode length: 13.03 +/- 2.58\n",
      "Eval num_timesteps=240000, episode_reward=16.85 +/- 2.42\n",
      "Episode length: 12.94 +/- 2.72\n",
      "  Mean reward (shaped): 16.76\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 5/5: {'learning_rate': 0.0001, 'batch_size': 64, 'buffer_size': 50000, 'gamma': 0.999, 'target_update_interval': 2000, 'exploration_fraction': 0.9, 'exploration_final_eps': 0.2, 'learning_starts': 20000, 'train_freq': 4, 'gradient_steps': 4}\n",
      "Eval num_timesteps=20000, episode_reward=-992.00 +/- 79.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-15.04 +/- 46.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=5.80 +/- 10.81\n",
      "Episode length: 124.32 +/- 92.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-74.94 +/- 102.41\n",
      "Episode length: 126.91 +/- 91.43\n",
      "Eval num_timesteps=100000, episode_reward=-22.27 +/- 62.85\n",
      "Episode length: 175.78 +/- 62.66\n",
      "Eval num_timesteps=120000, episode_reward=-3.32 +/- 1.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-3.40 +/- 1.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1.50 +/- 6.34\n",
      "Episode length: 184.83 +/- 51.45\n",
      "Eval num_timesteps=180000, episode_reward=-0.68 +/- 6.82\n",
      "Episode length: 179.35 +/- 58.75\n",
      "Eval num_timesteps=200000, episode_reward=1.74 +/- 8.95\n",
      "Episode length: 158.52 +/- 78.11\n",
      "Eval num_timesteps=220000, episode_reward=2.65 +/- 9.90\n",
      "Episode length: 145.47 +/- 85.33\n",
      "Eval num_timesteps=240000, episode_reward=4.56 +/- 10.34\n",
      "Episode length: 132.24 +/- 90.36\n",
      "  Mean reward (shaped): 9.71\n",
      "  Success rate (default): 57.00%\n",
      "\n",
      "Saved DQN tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_tuning_enhanced.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_enhanced_best.yaml\n",
      "\n",
      "Saved DQN tuning summary to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/dqn_per_reward/dqn_tuning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "summary_rows = []\n",
    "\n",
    "for reward_index, reward_name in enumerate(reward_functions):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"DQN tuning for reward: {reward_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    configs = random_sample_params(dqn_param_space, N_RANDOM_CONFIGS, seed=SEED + reward_index * 100)\n",
    "    reward_results = []\n",
    "\n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f'\\nConfiguration {i}/{N_RANDOM_CONFIGS}: {config}')\n",
    "        start_time = time.time()\n",
    "        env = None\n",
    "        eval_env_default = None\n",
    "\n",
    "        try:\n",
    "            env = make_taxi_env(\n",
    "                use_feature_wrapper=True,\n",
    "                reward_wrapper_name=reward_name,\n",
    "                use_action_masking=False,\n",
    "            )\n",
    "\n",
    "            agent = DQNAgent(\n",
    "                env=env,\n",
    "                policy=DQN_BASE_AGENT['policy'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                buffer_size=config['buffer_size'],\n",
    "                learning_starts=config['learning_starts'],\n",
    "                batch_size=config['batch_size'],\n",
    "                gamma=config['gamma'],\n",
    "                train_freq=config['train_freq'],\n",
    "                gradient_steps=config['gradient_steps'],\n",
    "                target_update_interval=config['target_update_interval'],\n",
    "                exploration_fraction=config['exploration_fraction'],\n",
    "                exploration_initial_eps=DQN_BASE_AGENT['exploration_initial_eps'],\n",
    "                exploration_final_eps=config['exploration_final_eps'],\n",
    "                policy_kwargs=DQN_BASE_AGENT.get('policy_kwargs') or {},\n",
    "                verbose=0,\n",
    "                seed=DQN_BASE_AGENT.get('seed'),\n",
    "            )\n",
    "\n",
    "            trainer = DQNTrainer(\n",
    "                env=env,\n",
    "                agent=agent,\n",
    "                log_dir=ROOT_DIR / f'results/logs/reward_tuning/dqn_{reward_name}/config_{i}',\n",
    "                eval_freq=20000,\n",
    "                eval_episodes=N_EVAL_EPISODES,\n",
    "            )\n",
    "            stats = trainer.train(total_timesteps=TRAINING_TIMESTEPS)\n",
    "\n",
    "            eval_shaped = evaluate_agent(agent, env, n_episodes=N_EVAL_EPISODES, deterministic=True)\n",
    "\n",
    "            eval_env_default = make_taxi_env(\n",
    "                use_feature_wrapper=True,\n",
    "                reward_wrapper_name=None,\n",
    "                use_action_masking=False,\n",
    "            )\n",
    "            eval_default = evaluate_agent(\n",
    "                agent,\n",
    "                eval_env_default,\n",
    "                n_episodes=N_EVAL_EPISODES,\n",
    "                deterministic=True,\n",
    "            )\n",
    "\n",
    "            training_time = stats.get('training_time', time.time() - start_time)\n",
    "\n",
    "            reward_results.append({\n",
    "                'config_id': i,\n",
    "                'reward_name': reward_name,\n",
    "                **config,\n",
    "                'mean_reward_shaped': eval_shaped['mean_reward'],\n",
    "                'std_reward_shaped': eval_shaped['std_reward'],\n",
    "                'success_rate_default': eval_default['success_rate'],\n",
    "                'mean_length_default': eval_default['mean_length'],\n",
    "                'training_time': training_time,\n",
    "            })\n",
    "\n",
    "            print(f\"  Mean reward (shaped): {eval_shaped['mean_reward']:.2f}\")\n",
    "            print(f\"  Success rate (default): {eval_default['success_rate']:.2%}\")\n",
    "        except Exception as exc:\n",
    "            print(f'  ERROR: {exc}')\n",
    "        finally:\n",
    "            if eval_env_default is not None:\n",
    "                eval_env_default.close()\n",
    "            if env is not None:\n",
    "                env.close()\n",
    "\n",
    "    reward_df = pd.DataFrame(reward_results)\n",
    "    reward_df.sort_values(by='mean_reward_shaped', ascending=False, inplace=True)\n",
    "\n",
    "    reward_csv = RESULTS_DIR / f'dqn_tuning_{reward_name}.csv'\n",
    "    reward_df.to_csv(reward_csv, index=False)\n",
    "    print(f'\\nSaved DQN tuning results to {reward_csv}')\n",
    "\n",
    "    if not reward_df.empty:\n",
    "        best_row = reward_df.iloc[0]\n",
    "        best_params = {k: best_row[k] for k in dqn_param_space.keys()}\n",
    "        best_yaml = RESULTS_DIR / f'dqn_{reward_name}_best.yaml'\n",
    "        save_optimized_config(\n",
    "            best_params=best_params,\n",
    "            base_config_path=ROOT_DIR / 'src/reinforcement_learning_taxi/configs/dqn_config_baseline.yaml',\n",
    "            output_path=best_yaml,\n",
    "            algorithm='DQN',\n",
    "        )\n",
    "        summary_rows.append({\n",
    "            'reward_name': reward_name,\n",
    "            'best_config_id': int(best_row['config_id']),\n",
    "            'best_mean_reward_shaped': float(best_row['mean_reward_shaped']),\n",
    "            'best_success_rate_default': float(best_row['success_rate_default']),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv = RESULTS_DIR / 'dqn_tuning_summary.csv'\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f'\\nSaved DQN tuning summary to {summary_csv}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
