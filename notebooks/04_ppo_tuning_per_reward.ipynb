{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Hyperparameter Tuning Per Reward Function\n",
    "\n",
    "This notebook tunes PPO hyperparameters separately for each reward function.\n",
    "Outputs are saved under `results/hyperparameter_tuning/ppo_per_reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reinforcement_learning_taxi.agents.ppo_agent import PPOAgent\n",
    "from reinforcement_learning_taxi.environments import make_taxi_env\n",
    "from reinforcement_learning_taxi.environments.reward_wrappers import REWARD_FUNCTIONS\n",
    "from reinforcement_learning_taxi.evaluation.metrics import evaluate_agent\n",
    "from reinforcement_learning_taxi.training.ppo_trainer import PPOTrainer\n",
    "from reinforcement_learning_taxi.utils.config_utils import load_config, save_optimized_config\n",
    "from reinforcement_learning_taxi.utils.path_utils import get_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward functions: ['default', 'distance_based', 'modified_penalty', 'enhanced']\n",
      "Training timesteps: 200,000\n",
      "Random configs per reward: 20\n",
      "Evaluation episodes: 200\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = get_repo_root()\n",
    "RESULTS_DIR = ROOT_DIR / 'results/hyperparameter_tuning/ppo_per_reward'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PPO_BASE_CONFIG = load_config(ROOT_DIR / 'src/reinforcement_learning_taxi/configs/ppo_config_baseline.yaml')\n",
    "PPO_BASE_AGENT = PPO_BASE_CONFIG['agent']\n",
    "\n",
    "TRAINING_TIMESTEPS = 200000\n",
    "N_EVAL_EPISODES = 200\n",
    "N_RANDOM_CONFIGS = 20\n",
    "SEED = 42\n",
    "\n",
    "reward_functions = list(REWARD_FUNCTIONS.keys())\n",
    "print(f'Reward functions: {reward_functions}')\n",
    "print(f'Training timesteps: {TRAINING_TIMESTEPS:,}')\n",
    "print(f'Random configs per reward: {N_RANDOM_CONFIGS}')\n",
    "print(f'Evaluation episodes: {N_EVAL_EPISODES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Search Space:\n",
      "  learning_rate: [0.0001, 0.0003, 0.0005, 0.001]\n",
      "  n_steps: [512, 1024, 2048]\n",
      "  batch_size: [32, 64, 128]\n",
      "  clip_range: [0.1, 0.2, 0.3]\n",
      "  ent_coef: [0.0, 0.01, 0.05]\n",
      "Total possible combinations: 324\n"
     ]
    }
   ],
   "source": [
    "ppo_param_space = {\n",
    "    'learning_rate': [1e-4, 3e-4, 5e-4, 1e-3],\n",
    "    'n_steps': [512, 1024, 2048],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'clip_range': [0.1, 0.2, 0.3],\n",
    "    'ent_coef': [0.0, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "print('PPO Search Space:')\n",
    "for param, values in ppo_param_space.items():\n",
    "    print(f'  {param}: {values}')\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in ppo_param_space.values()])\n",
    "print(f'Total possible combinations: {total_combinations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_params(param_space, n_samples, seed=42):\n",
    "    random.seed(seed)\n",
    "    configs = []\n",
    "    for _ in range(n_samples):\n",
    "        config = {param: random.choice(values) for param, values in param_space.items()}\n",
    "        configs.append(config)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PPO tuning for reward: default\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-195.72 +/- 29.96\n",
      "Episode length: 196.14 +/- 27.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-195.75 +/- 29.75\n",
      "Episode length: 196.17 +/- 26.81\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-191.63 +/- 41.01\n",
      "Episode length: 192.47 +/- 36.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-170.96 +/- 71.98\n",
      "Episode length: 173.90 +/- 64.69\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-162.44 +/- 80.18\n",
      "Episode length: 166.22 +/- 72.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-81.34 +/- 103.08\n",
      "Episode length: 93.31 +/- 92.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-121.24 +/- 100.61\n",
      "Episode length: 129.22 +/- 90.42\n",
      "Eval num_timesteps=180000, episode_reward=-83.43 +/- 103.35\n",
      "Episode length: 95.19 +/- 92.93\n",
      "Eval num_timesteps=200000, episode_reward=-10.93 +/- 59.51\n",
      "Episode length: 30.04 +/- 53.50\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -10.56\n",
      "  Success rate (default): 90.00%\n",
      "\n",
      "Configuration 2/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-191.62 +/- 41.05\n",
      "Episode length: 192.46 +/- 36.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-193.64 +/- 36.17\n",
      "Episode length: 194.27 +/- 32.58\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-73.41 +/- 101.25\n",
      "Episode length: 86.22 +/- 91.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-44.53 +/- 89.79\n",
      "Episode length: 60.28 +/- 80.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=5.56 +/- 20.83\n",
      "Episode length: 15.23 +/- 18.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=8.05 +/- 2.73\n",
      "Episode length: 12.95 +/- 2.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=7.40 +/- 2.53\n",
      "Episode length: 13.60 +/- 2.53\n",
      "Eval num_timesteps=180000, episode_reward=8.25 +/- 2.61\n",
      "Episode length: 12.75 +/- 2.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=5.66 +/- 20.87\n",
      "Episode length: 15.13 +/- 18.80\n",
      "  Mean reward (shaped): 6.96\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 3/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-195.84 +/- 29.12\n",
      "Episode length: 196.26 +/- 26.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-181.12 +/- 60.04\n",
      "Episode length: 183.01 +/- 54.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-158.14 +/- 83.73\n",
      "Episode length: 162.34 +/- 75.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-124.84 +/- 100.23\n",
      "Episode length: 132.40 +/- 90.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-69.42 +/- 100.10\n",
      "Episode length: 82.65 +/- 89.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-27.50 +/- 78.11\n",
      "Episode length: 44.93 +/- 70.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-18.74 +/- 70.11\n",
      "Episode length: 37.01 +/- 63.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=1.49 +/- 35.55\n",
      "Episode length: 18.88 +/- 31.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=5.58 +/- 20.82\n",
      "Episode length: 15.21 +/- 18.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=8.13 +/- 2.86\n",
      "Episode length: 12.87 +/- 2.86\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 7.71\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 4/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-197.88 +/- 21.09\n",
      "Episode length: 198.09 +/- 19.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-178.99 +/- 63.04\n",
      "Episode length: 181.09 +/- 56.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-135.38 +/- 96.42\n",
      "Episode length: 141.89 +/- 86.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-6.68 +/- 53.09\n",
      "Episode length: 26.21 +/- 47.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-23.35 +/- 74.24\n",
      "Episode length: 41.20 +/- 66.74\n",
      "Eval num_timesteps=140000, episode_reward=-11.04 +/- 59.49\n",
      "Episode length: 30.15 +/- 53.49\n",
      "Eval num_timesteps=160000, episode_reward=8.22 +/- 3.14\n",
      "Episode length: 12.78 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=7.61 +/- 2.49\n",
      "Episode length: 13.39 +/- 2.49\n",
      "Eval num_timesteps=200000, episode_reward=-2.13 +/- 45.48\n",
      "Episode length: 22.08 +/- 40.91\n",
      "  Mean reward (shaped): 2.42\n",
      "  Success rate (default): 99.00%\n",
      "\n",
      "Configuration 5/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-193.77 +/- 35.43\n",
      "Episode length: 194.40 +/- 31.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-76.68 +/- 102.82\n",
      "Episode length: 89.07 +/- 92.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-4.82 +/- 49.38\n",
      "Episode length: 24.56 +/- 44.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=7.74 +/- 2.33\n",
      "Episode length: 13.26 +/- 2.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=1.58 +/- 35.53\n",
      "Episode length: 18.79 +/- 31.96\n",
      "Eval num_timesteps=160000, episode_reward=7.70 +/- 2.58\n",
      "Episode length: 13.30 +/- 2.58\n",
      "Eval num_timesteps=180000, episode_reward=3.57 +/- 29.18\n",
      "Episode length: 17.01 +/- 26.25\n",
      "Eval num_timesteps=200000, episode_reward=7.54 +/- 2.85\n",
      "Episode length: 13.46 +/- 2.85\n",
      "  Mean reward (shaped): 5.46\n",
      "  Success rate (default): 98.00%\n",
      "\n",
      "Configuration 6/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-195.86 +/- 28.98\n",
      "Episode length: 196.28 +/- 26.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "  Mean reward (shaped): -200.00\n",
      "  Success rate (default): 0.00%\n",
      "\n",
      "Configuration 7/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-197.86 +/- 21.29\n",
      "Episode length: 198.07 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-178.91 +/- 63.27\n",
      "Episode length: 181.01 +/- 56.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-175.28 +/- 66.94\n",
      "Episode length: 177.80 +/- 60.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-155.78 +/- 85.77\n",
      "Episode length: 160.19 +/- 77.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-145.74 +/- 91.55\n",
      "Episode length: 151.20 +/- 82.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-170.80 +/- 72.38\n",
      "Episode length: 173.74 +/- 65.10\n",
      "Eval num_timesteps=160000, episode_reward=-58.36 +/- 97.20\n",
      "Episode length: 72.64 +/- 87.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-69.75 +/- 99.84\n",
      "Episode length: 82.98 +/- 89.70\n",
      "Eval num_timesteps=200000, episode_reward=-2.95 +/- 45.30\n",
      "Episode length: 22.90 +/- 40.74\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -8.23\n",
      "  Success rate (default): 90.00%\n",
      "\n",
      "Configuration 8/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-197.86 +/- 21.29\n",
      "Episode length: 198.07 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "  Mean reward (shaped): -200.00\n",
      "  Success rate (default): 0.00%\n",
      "\n",
      "Configuration 9/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-195.76 +/- 29.68\n",
      "Episode length: 196.18 +/- 26.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-164.41 +/- 78.65\n",
      "Episode length: 167.98 +/- 70.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-108.23 +/- 103.55\n",
      "Episode length: 117.47 +/- 93.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-96.09 +/- 103.92\n",
      "Episode length: 106.59 +/- 93.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-40.79 +/- 87.05\n",
      "Episode length: 56.96 +/- 78.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=3.40 +/- 29.24\n",
      "Episode length: 17.18 +/- 26.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=7.19 +/- 2.87\n",
      "Episode length: 13.81 +/- 2.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-46.61 +/- 90.95\n",
      "Episode length: 62.15 +/- 81.74\n",
      "Eval num_timesteps=180000, episode_reward=-9.34 +/- 56.28\n",
      "Episode length: 28.66 +/- 50.59\n",
      "Eval num_timesteps=200000, episode_reward=-90.30 +/- 103.33\n",
      "Episode length: 101.43 +/- 92.85\n",
      "  Mean reward (shaped): -83.81\n",
      "  Success rate (default): 57.00%\n",
      "\n",
      "Configuration 10/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-195.70 +/- 30.10\n",
      "Episode length: 196.12 +/- 27.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-197.86 +/- 21.29\n",
      "Episode length: 198.07 +/- 19.20\n",
      "Eval num_timesteps=100000, episode_reward=-193.77 +/- 35.43\n",
      "Episode length: 194.40 +/- 31.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-75.33 +/- 101.81\n",
      "Episode length: 87.93 +/- 91.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=5.27 +/- 20.81\n",
      "Episode length: 15.52 +/- 18.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-23.40 +/- 74.22\n",
      "Episode length: 41.25 +/- 66.73\n",
      "Eval num_timesteps=180000, episode_reward=7.59 +/- 2.60\n",
      "Episode length: 13.41 +/- 2.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.33 +/- 40.84\n",
      "Episode length: 20.49 +/- 36.74\n",
      "  Mean reward (shaped): 7.89\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 11/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-149.91 +/- 89.15\n",
      "Episode length: 154.95 +/- 80.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-142.10 +/- 92.86\n",
      "Episode length: 147.98 +/- 83.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-85.83 +/- 103.28\n",
      "Episode length: 97.38 +/- 92.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-81.94 +/- 102.56\n",
      "Episode length: 93.91 +/- 92.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-97.97 +/- 104.11\n",
      "Episode length: 108.26 +/- 93.61\n",
      "Eval num_timesteps=180000, episode_reward=-110.72 +/- 102.80\n",
      "Episode length: 119.75 +/- 92.41\n",
      "Eval num_timesteps=200000, episode_reward=-75.34 +/- 101.81\n",
      "Episode length: 87.94 +/- 91.52\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -60.64\n",
      "  Success rate (default): 68.00%\n",
      "\n",
      "Configuration 12/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-187.43 +/- 49.75\n",
      "Episode length: 188.69 +/- 44.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-141.66 +/- 93.56\n",
      "Episode length: 147.54 +/- 84.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-127.47 +/- 98.87\n",
      "Episode length: 134.82 +/- 88.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-122.97 +/- 100.53\n",
      "Episode length: 130.74 +/- 90.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-84.83 +/- 104.19\n",
      "Episode length: 96.38 +/- 93.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-88.15 +/- 103.26\n",
      "Episode length: 99.49 +/- 92.79\n",
      "Eval num_timesteps=160000, episode_reward=-106.57 +/- 103.31\n",
      "Episode length: 116.02 +/- 92.87\n",
      "Eval num_timesteps=180000, episode_reward=-123.28 +/- 100.12\n",
      "Episode length: 131.05 +/- 89.99\n",
      "Eval num_timesteps=200000, episode_reward=-81.42 +/- 103.01\n",
      "Episode length: 93.39 +/- 92.61\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -73.60\n",
      "  Success rate (default): 64.50%\n",
      "\n",
      "Configuration 13/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-162.52 +/- 80.02\n",
      "Episode length: 166.30 +/- 71.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-158.29 +/- 83.43\n",
      "Episode length: 162.49 +/- 75.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-152.03 +/- 87.79\n",
      "Episode length: 156.86 +/- 78.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-151.88 +/- 88.05\n",
      "Episode length: 156.71 +/- 79.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-156.27 +/- 84.83\n",
      "Episode length: 160.68 +/- 76.28\n",
      "Eval num_timesteps=160000, episode_reward=-110.58 +/- 102.96\n",
      "Episode length: 119.61 +/- 92.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-88.10 +/- 103.31\n",
      "Episode length: 99.44 +/- 92.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-83.71 +/- 103.10\n",
      "Episode length: 95.47 +/- 92.68\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -83.51\n",
      "  Success rate (default): 48.00%\n",
      "\n",
      "Configuration 14/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-173.03 +/- 69.77\n",
      "Episode length: 175.76 +/- 62.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-193.68 +/- 35.94\n",
      "Episode length: 194.31 +/- 32.35\n",
      "Eval num_timesteps=140000, episode_reward=-137.37 +/- 95.68\n",
      "Episode length: 143.67 +/- 86.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-104.44 +/- 103.55\n",
      "Episode length: 114.10 +/- 93.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-75.22 +/- 101.90\n",
      "Episode length: 87.82 +/- 91.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-73.13 +/- 101.47\n",
      "Episode length: 85.94 +/- 91.22\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -172.00\n",
      "  Success rate (default): 10.00%\n",
      "\n",
      "Configuration 15/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-193.64 +/- 36.16\n",
      "Episode length: 194.27 +/- 32.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-154.49 +/- 85.72\n",
      "Episode length: 159.11 +/- 77.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-107.21 +/- 102.64\n",
      "Episode length: 116.66 +/- 92.20\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -124.61\n",
      "  Success rate (default): 25.50%\n",
      "\n",
      "Configuration 16/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-189.44 +/- 46.03\n",
      "Episode length: 190.49 +/- 41.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-143.40 +/- 93.08\n",
      "Episode length: 149.07 +/- 83.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-137.02 +/- 96.21\n",
      "Episode length: 143.32 +/- 86.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-16.52 +/- 67.79\n",
      "Episode length: 35.00 +/- 60.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-11.06 +/- 59.48\n",
      "Episode length: 30.17 +/- 53.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-72.98 +/- 101.59\n",
      "Episode length: 85.79 +/- 91.35\n",
      "Eval num_timesteps=160000, episode_reward=7.58 +/- 2.78\n",
      "Episode length: 13.42 +/- 2.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-125.04 +/- 99.96\n",
      "Episode length: 132.60 +/- 89.88\n",
      "Eval num_timesteps=200000, episode_reward=-21.53 +/- 72.05\n",
      "Episode length: 39.59 +/- 64.76\n",
      "  Mean reward (shaped): -149.00\n",
      "  Success rate (default): 24.50%\n",
      "\n",
      "Configuration 17/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-187.30 +/- 50.27\n",
      "Episode length: 188.56 +/- 45.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-191.49 +/- 41.69\n",
      "Episode length: 192.33 +/- 37.58\n",
      "Eval num_timesteps=60000, episode_reward=-135.54 +/- 96.18\n",
      "Episode length: 142.05 +/- 86.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-145.85 +/- 91.36\n",
      "Episode length: 151.31 +/- 82.15\n",
      "Eval num_timesteps=100000, episode_reward=8.03 +/- 2.80\n",
      "Episode length: 12.97 +/- 2.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=7.86 +/- 2.24\n",
      "Episode length: 13.14 +/- 2.24\n",
      "Eval num_timesteps=140000, episode_reward=8.16 +/- 2.56\n",
      "Episode length: 12.84 +/- 2.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=7.97 +/- 2.66\n",
      "Episode length: 13.03 +/- 2.66\n",
      "Eval num_timesteps=180000, episode_reward=7.74 +/- 2.42\n",
      "Episode length: 13.26 +/- 2.42\n",
      "Eval num_timesteps=200000, episode_reward=7.71 +/- 2.26\n",
      "Episode length: 13.29 +/- 2.26\n",
      "  Mean reward (shaped): 7.93\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 18/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-195.77 +/- 29.61\n",
      "Episode length: 196.19 +/- 26.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-158.35 +/- 83.32\n",
      "Episode length: 162.55 +/- 74.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-62.49 +/- 98.72\n",
      "Episode length: 76.35 +/- 88.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-85.67 +/- 103.43\n",
      "Episode length: 97.22 +/- 92.98\n",
      "Eval num_timesteps=100000, episode_reward=-123.43 +/- 99.93\n",
      "Episode length: 131.20 +/- 89.79\n",
      "Eval num_timesteps=120000, episode_reward=-25.00 +/- 76.42\n",
      "Episode length: 42.64 +/- 68.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-65.01 +/- 99.09\n",
      "Episode length: 78.66 +/- 89.07\n",
      "Eval num_timesteps=160000, episode_reward=-19.54 +/- 69.80\n",
      "Episode length: 37.81 +/- 62.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-97.86 +/- 104.22\n",
      "Episode length: 108.15 +/- 93.73\n",
      "Eval num_timesteps=200000, episode_reward=1.83 +/- 35.58\n",
      "Episode length: 18.54 +/- 32.00\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -165.41\n",
      "  Success rate (default): 16.50%\n",
      "\n",
      "Configuration 19/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-193.64 +/- 36.16\n",
      "Episode length: 194.27 +/- 32.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-191.72 +/- 40.57\n",
      "Episode length: 192.56 +/- 36.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-118.69 +/- 101.71\n",
      "Episode length: 126.88 +/- 91.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-7.11 +/- 52.98\n",
      "Episode length: 26.64 +/- 47.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=8.04 +/- 2.83\n",
      "Episode length: 12.96 +/- 2.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=7.74 +/- 2.70\n",
      "Episode length: 13.26 +/- 2.70\n",
      "Eval num_timesteps=140000, episode_reward=-46.31 +/- 91.12\n",
      "Episode length: 61.85 +/- 81.91\n",
      "Eval num_timesteps=160000, episode_reward=-81.23 +/- 103.18\n",
      "Episode length: 93.20 +/- 92.78\n",
      "Eval num_timesteps=180000, episode_reward=-16.80 +/- 67.68\n",
      "Episode length: 35.28 +/- 60.86\n",
      "Eval num_timesteps=200000, episode_reward=-56.18 +/- 96.43\n",
      "Episode length: 70.67 +/- 86.72\n",
      "  Mean reward (shaped): -36.95\n",
      "  Success rate (default): 73.00%\n",
      "\n",
      "Configuration 20/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-162.32 +/- 80.43\n",
      "Episode length: 166.10 +/- 72.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-146.12 +/- 90.91\n",
      "Episode length: 151.58 +/- 81.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-140.00 +/- 93.90\n",
      "Episode length: 146.09 +/- 84.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-124.71 +/- 100.40\n",
      "Episode length: 132.27 +/- 90.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-41.65 +/- 89.01\n",
      "Episode length: 57.61 +/- 80.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=7.50 +/- 2.57\n",
      "Episode length: 13.50 +/- 2.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.19 +/- 2.96\n",
      "Episode length: 12.81 +/- 2.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=7.87 +/- 2.46\n",
      "Episode length: 13.13 +/- 2.46\n",
      "Eval num_timesteps=200000, episode_reward=-41.93 +/- 88.85\n",
      "Episode length: 57.89 +/- 79.88\n",
      "  Mean reward (shaped): 7.75\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Saved PPO tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_tuning_default.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_default_best.yaml\n",
      "\n",
      "======================================================================\n",
      "PPO tuning for reward: distance_based\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-199.92 +/- 0.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.91 +/- 0.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-199.87 +/- 0.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-199.93 +/- 0.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-199.91 +/- 0.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-189.32 +/- 45.87\n",
      "Episode length: 190.56 +/- 41.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-187.17 +/- 49.90\n",
      "Episode length: 188.68 +/- 44.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-199.80 +/- 0.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-97.67 +/- 104.24\n",
      "Episode length: 108.45 +/- 93.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-55.58 +/- 96.61\n",
      "Episode length: 70.78 +/- 86.64\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -111.04\n",
      "  Success rate (default): 43.50%\n",
      "\n",
      "Configuration 2/20: {'learning_rate': 0.0005, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-199.74 +/- 0.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-51.56 +/- 94.80\n",
      "Episode length: 67.17 +/- 84.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-32.89 +/- 83.52\n",
      "Episode length: 50.45 +/- 74.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=4.18 +/- 29.24\n",
      "Episode length: 17.30 +/- 26.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=8.75 +/- 2.51\n",
      "Episode length: 13.13 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-45.19 +/- 91.30\n",
      "Episode length: 61.63 +/- 82.04\n",
      "Eval num_timesteps=140000, episode_reward=-1.79 +/- 45.51\n",
      "Episode length: 22.64 +/- 40.76\n",
      "Eval num_timesteps=160000, episode_reward=4.35 +/- 29.28\n",
      "Episode length: 17.17 +/- 26.23\n",
      "Eval num_timesteps=180000, episode_reward=8.81 +/- 2.34\n",
      "Episode length: 13.08 +/- 2.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-90.77 +/- 104.40\n",
      "Episode length: 102.43 +/- 93.76\n",
      "  Mean reward (shaped): -54.67\n",
      "  Success rate (default): 68.00%\n",
      "\n",
      "Configuration 3/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.71 +/- 0.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-168.25 +/- 75.13\n",
      "Episode length: 171.68 +/- 67.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-95.43 +/- 104.34\n",
      "Episode length: 106.57 +/- 93.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.77 +/- 41.04\n",
      "Episode length: 20.25 +/- 36.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-39.19 +/- 87.51\n",
      "Episode length: 56.24 +/- 78.60\n",
      "Eval num_timesteps=120000, episode_reward=9.22 +/- 2.54\n",
      "Episode length: 12.66 +/- 2.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=9.22 +/- 2.39\n",
      "Episode length: 12.71 +/- 2.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-51.62 +/- 94.64\n",
      "Episode length: 67.28 +/- 84.84\n",
      "Eval num_timesteps=180000, episode_reward=8.95 +/- 2.73\n",
      "Episode length: 12.96 +/- 2.96\n",
      "Eval num_timesteps=200000, episode_reward=-49.94 +/- 93.21\n",
      "Episode length: 65.87 +/- 83.68\n",
      "  Mean reward (shaped): 8.85\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 4/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.85 +/- 0.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-191.33 +/- 41.47\n",
      "Episode length: 192.40 +/- 37.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-170.18 +/- 73.05\n",
      "Episode length: 173.53 +/- 65.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-106.13 +/- 103.48\n",
      "Episode length: 116.19 +/- 92.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-58.20 +/- 97.22\n",
      "Episode length: 73.15 +/- 87.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=2.45 +/- 35.69\n",
      "Episode length: 18.77 +/- 31.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=4.57 +/- 29.32\n",
      "Episode length: 16.91 +/- 26.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.74 +/- 2.40\n",
      "Episode length: 13.15 +/- 2.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=8.42 +/- 2.26\n",
      "Episode length: 13.52 +/- 2.41\n",
      "Eval num_timesteps=200000, episode_reward=8.67 +/- 2.32\n",
      "Episode length: 13.20 +/- 2.43\n",
      "  Mean reward (shaped): 8.71\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 5/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-199.96 +/- 0.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-168.23 +/- 75.10\n",
      "Episode length: 171.70 +/- 67.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-132.83 +/- 97.48\n",
      "Episode length: 140.08 +/- 87.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-53.28 +/- 95.94\n",
      "Episode length: 68.70 +/- 85.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.27 +/- 29.27\n",
      "Episode length: 17.27 +/- 26.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.61 +/- 2.48\n",
      "Episode length: 13.29 +/- 2.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-32.52 +/- 83.56\n",
      "Episode length: 50.11 +/- 74.99\n",
      "Eval num_timesteps=160000, episode_reward=-37.24 +/- 86.36\n",
      "Episode length: 54.36 +/- 77.38\n",
      "Eval num_timesteps=180000, episode_reward=-26.65 +/- 78.34\n",
      "Episode length: 44.91 +/- 70.23\n",
      "Eval num_timesteps=200000, episode_reward=2.58 +/- 35.68\n",
      "Episode length: 18.66 +/- 31.97\n",
      "  Mean reward (shaped): -9.68\n",
      "  Success rate (default): 89.50%\n",
      "\n",
      "Configuration 6/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-199.69 +/- 0.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-44.92 +/- 91.74\n",
      "Episode length: 61.24 +/- 82.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-84.85 +/- 103.70\n",
      "Episode length: 97.14 +/- 93.06\n",
      "Eval num_timesteps=80000, episode_reward=-138.54 +/- 95.17\n",
      "Episode length: 145.34 +/- 85.53\n",
      "Eval num_timesteps=100000, episode_reward=6.49 +/- 20.87\n",
      "Episode length: 15.20 +/- 18.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-1.73 +/- 45.49\n",
      "Episode length: 22.57 +/- 40.76\n",
      "Eval num_timesteps=140000, episode_reward=-16.07 +/- 67.87\n",
      "Episode length: 35.36 +/- 60.85\n",
      "Eval num_timesteps=160000, episode_reward=-95.89 +/- 103.64\n",
      "Episode length: 107.09 +/- 92.93\n",
      "Eval num_timesteps=180000, episode_reward=-138.75 +/- 94.94\n",
      "Episode length: 145.47 +/- 85.33\n",
      "Eval num_timesteps=200000, episode_reward=-8.25 +/- 56.53\n",
      "Episode length: 28.46 +/- 50.64\n",
      "  Mean reward (shaped): -14.87\n",
      "  Success rate (default): 93.00%\n",
      "\n",
      "Configuration 7/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.89 +/- 0.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-195.61 +/- 29.50\n",
      "Episode length: 196.22 +/- 26.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-170.45 +/- 72.27\n",
      "Episode length: 173.89 +/- 64.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-72.34 +/- 101.98\n",
      "Episode length: 85.81 +/- 91.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-1.86 +/- 45.51\n",
      "Episode length: 22.71 +/- 40.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-10.32 +/- 59.67\n",
      "Episode length: 30.31 +/- 53.43\n",
      "Eval num_timesteps=140000, episode_reward=0.31 +/- 40.92\n",
      "Episode length: 20.74 +/- 36.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.65 +/- 2.48\n",
      "Episode length: 13.24 +/- 2.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=8.46 +/- 2.43\n",
      "Episode length: 13.45 +/- 2.57\n",
      "Eval num_timesteps=200000, episode_reward=8.74 +/- 2.27\n",
      "Episode length: 13.12 +/- 2.35\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 8.49\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 8/20: {'learning_rate': 0.0005, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-157.79 +/- 83.83\n",
      "Episode length: 162.42 +/- 75.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-57.61 +/- 97.40\n",
      "Episode length: 72.65 +/- 87.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=9.18 +/- 2.65\n",
      "Episode length: 12.66 +/- 2.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-24.87 +/- 76.32\n",
      "Episode length: 43.37 +/- 68.39\n",
      "Eval num_timesteps=100000, episode_reward=-18.42 +/- 70.22\n",
      "Episode length: 37.46 +/- 62.88\n",
      "Eval num_timesteps=120000, episode_reward=-16.15 +/- 67.92\n",
      "Episode length: 35.43 +/- 60.81\n",
      "Eval num_timesteps=140000, episode_reward=-9.54 +/- 59.85\n",
      "Episode length: 29.48 +/- 53.69\n",
      "Eval num_timesteps=160000, episode_reward=6.55 +/- 20.90\n",
      "Episode length: 15.15 +/- 18.75\n",
      "Eval num_timesteps=180000, episode_reward=-9.95 +/- 59.71\n",
      "Episode length: 29.90 +/- 53.54\n",
      "Eval num_timesteps=200000, episode_reward=-97.09 +/- 104.35\n",
      "Episode length: 108.14 +/- 93.74\n",
      "  Mean reward (shaped): -70.38\n",
      "  Success rate (default): 65.00%\n",
      "\n",
      "Configuration 9/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-199.91 +/- 0.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-172.30 +/- 71.13\n",
      "Episode length: 175.31 +/- 63.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-132.73 +/- 97.66\n",
      "Episode length: 139.96 +/- 87.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-69.99 +/- 101.61\n",
      "Episode length: 83.68 +/- 91.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=0.33 +/- 40.98\n",
      "Episode length: 20.74 +/- 36.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.90 +/- 2.40\n",
      "Episode length: 13.00 +/- 2.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-24.02 +/- 76.67\n",
      "Episode length: 42.47 +/- 68.80\n",
      "Eval num_timesteps=160000, episode_reward=-35.26 +/- 84.88\n",
      "Episode length: 52.60 +/- 76.03\n",
      "Eval num_timesteps=180000, episode_reward=-24.46 +/- 76.45\n",
      "Episode length: 42.93 +/- 68.60\n",
      "Eval num_timesteps=200000, episode_reward=8.71 +/- 2.27\n",
      "Episode length: 13.19 +/- 2.38\n",
      "  Mean reward (shaped): -6.56\n",
      "  Success rate (default): 89.00%\n",
      "\n",
      "Configuration 10/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.82 +/- 0.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-149.66 +/- 89.22\n",
      "Episode length: 155.11 +/- 79.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=6.12 +/- 20.90\n",
      "Episode length: 15.51 +/- 18.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=8.81 +/- 2.44\n",
      "Episode length: 13.09 +/- 2.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-15.90 +/- 67.88\n",
      "Episode length: 35.19 +/- 60.92\n",
      "Eval num_timesteps=120000, episode_reward=-3.68 +/- 49.63\n",
      "Episode length: 24.26 +/- 44.47\n",
      "Eval num_timesteps=140000, episode_reward=-18.46 +/- 70.19\n",
      "Episode length: 37.57 +/- 62.85\n",
      "Eval num_timesteps=160000, episode_reward=-38.62 +/- 88.05\n",
      "Episode length: 55.53 +/- 78.99\n",
      "Eval num_timesteps=180000, episode_reward=6.92 +/- 20.92\n",
      "Episode length: 14.77 +/- 18.79\n",
      "Eval num_timesteps=200000, episode_reward=8.51 +/- 2.30\n",
      "Episode length: 13.40 +/- 2.45\n",
      "  Mean reward (shaped): 8.81\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 11/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-191.39 +/- 41.29\n",
      "Episode length: 192.44 +/- 37.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.75 +/- 0.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-184.82 +/- 54.05\n",
      "Episode length: 186.67 +/- 48.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-147.47 +/- 90.31\n",
      "Episode length: 153.27 +/- 80.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-70.33 +/- 101.33\n",
      "Episode length: 84.02 +/- 90.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-45.07 +/- 91.72\n",
      "Episode length: 61.38 +/- 82.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-9.86 +/- 59.85\n",
      "Episode length: 29.84 +/- 53.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-8.05 +/- 56.62\n",
      "Episode length: 28.29 +/- 50.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-3.38 +/- 49.73\n",
      "Episode length: 23.93 +/- 44.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-9.94 +/- 59.81\n",
      "Episode length: 29.89 +/- 53.56\n",
      "  Mean reward (shaped): -6.79\n",
      "  Success rate (default): 93.50%\n",
      "\n",
      "Configuration 12/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-195.60 +/- 29.82\n",
      "Episode length: 196.17 +/- 26.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-185.01 +/- 53.96\n",
      "Episode length: 186.71 +/- 48.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-18.27 +/- 70.27\n",
      "Episode length: 37.35 +/- 62.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-3.91 +/- 49.59\n",
      "Episode length: 24.51 +/- 44.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=2.75 +/- 35.73\n",
      "Episode length: 18.49 +/- 32.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=6.40 +/- 20.88\n",
      "Episode length: 15.30 +/- 18.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-1.26 +/- 45.66\n",
      "Episode length: 22.05 +/- 40.91\n",
      "Eval num_timesteps=160000, episode_reward=9.07 +/- 2.37\n",
      "Episode length: 12.86 +/- 2.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=4.88 +/- 29.37\n",
      "Episode length: 16.58 +/- 26.34\n",
      "Eval num_timesteps=200000, episode_reward=0.79 +/- 41.05\n",
      "Episode length: 20.19 +/- 36.79\n",
      "  Mean reward (shaped): 5.45\n",
      "  Success rate (default): 97.50%\n",
      "\n",
      "Configuration 13/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.65 +/- 0.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.74 +/- 0.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-102.01 +/- 103.80\n",
      "Episode length: 112.44 +/- 93.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-58.01 +/- 97.33\n",
      "Episode length: 72.99 +/- 87.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-2.58 +/- 45.37\n",
      "Episode length: 23.42 +/- 40.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.14 +/- 2.98\n",
      "Episode length: 13.77 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=4.53 +/- 29.31\n",
      "Episode length: 16.91 +/- 26.27\n",
      "Eval num_timesteps=160000, episode_reward=8.28 +/- 2.43\n",
      "Episode length: 13.64 +/- 2.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=8.42 +/- 2.35\n",
      "Episode length: 13.51 +/- 2.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-30.72 +/- 81.85\n",
      "Episode length: 48.54 +/- 73.39\n",
      "  Mean reward (shaped): -31.79\n",
      "  Success rate (default): 76.50%\n",
      "\n",
      "Configuration 14/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.80 +/- 0.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-172.49 +/- 70.57\n",
      "Episode length: 175.55 +/- 63.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-153.69 +/- 86.64\n",
      "Episode length: 158.78 +/- 77.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-126.51 +/- 99.71\n",
      "Episode length: 134.44 +/- 89.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-39.20 +/- 87.84\n",
      "Episode length: 56.12 +/- 78.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=6.56 +/- 20.88\n",
      "Episode length: 15.13 +/- 18.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=8.35 +/- 2.43\n",
      "Episode length: 13.54 +/- 2.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.70 +/- 2.47\n",
      "Episode length: 13.22 +/- 2.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-47.13 +/- 92.63\n",
      "Episode length: 63.25 +/- 83.20\n",
      "Eval num_timesteps=200000, episode_reward=8.27 +/- 2.36\n",
      "Episode length: 13.65 +/- 2.47\n",
      "  Mean reward (shaped): 8.86\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 15/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-197.69 +/- 21.33\n",
      "Episode length: 198.07 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.78 +/- 0.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-199.85 +/- 0.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-170.50 +/- 72.75\n",
      "Episode length: 173.67 +/- 65.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-124.73 +/- 100.16\n",
      "Episode length: 132.72 +/- 89.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-131.00 +/- 98.17\n",
      "Episode length: 138.31 +/- 87.92\n",
      "Eval num_timesteps=140000, episode_reward=-135.05 +/- 96.64\n",
      "Episode length: 141.95 +/- 86.62\n",
      "Eval num_timesteps=160000, episode_reward=-88.83 +/- 104.50\n",
      "Episode length: 100.46 +/- 93.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-63.02 +/- 98.19\n",
      "Episode length: 77.56 +/- 87.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-43.51 +/- 90.19\n",
      "Episode length: 60.00 +/- 80.87\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -74.84\n",
      "  Success rate (default): 61.00%\n",
      "\n",
      "Configuration 16/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-199.81 +/- 0.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-199.96 +/- 0.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-199.91 +/- 0.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-193.44 +/- 36.47\n",
      "Episode length: 194.23 +/- 32.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-197.68 +/- 21.15\n",
      "Episode length: 198.09 +/- 19.00\n",
      "Eval num_timesteps=120000, episode_reward=-195.61 +/- 29.95\n",
      "Episode length: 196.15 +/- 26.95\n",
      "Eval num_timesteps=140000, episode_reward=-105.55 +/- 104.22\n",
      "Episode length: 115.50 +/- 93.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-151.57 +/- 88.16\n",
      "Episode length: 156.77 +/- 79.11\n",
      "Eval num_timesteps=180000, episode_reward=-137.45 +/- 95.06\n",
      "Episode length: 144.25 +/- 85.18\n",
      "Eval num_timesteps=200000, episode_reward=-46.11 +/- 91.10\n",
      "Episode length: 62.44 +/- 81.59\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -20.12\n",
      "  Success rate (default): 86.00%\n",
      "\n",
      "Configuration 17/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-159.90 +/- 82.43\n",
      "Episode length: 164.20 +/- 73.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-182.85 +/- 57.30\n",
      "Episode length: 184.83 +/- 51.45\n",
      "Eval num_timesteps=60000, episode_reward=-130.63 +/- 98.37\n",
      "Episode length: 138.10 +/- 88.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-59.90 +/- 98.14\n",
      "Episode length: 74.69 +/- 87.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-8.37 +/- 56.55\n",
      "Episode length: 28.58 +/- 50.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-30.65 +/- 81.95\n",
      "Episode length: 48.40 +/- 73.46\n",
      "Eval num_timesteps=140000, episode_reward=8.68 +/- 2.63\n",
      "Episode length: 13.22 +/- 2.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=8.54 +/- 2.31\n",
      "Episode length: 13.37 +/- 2.47\n",
      "Eval num_timesteps=180000, episode_reward=-26.61 +/- 78.33\n",
      "Episode length: 44.87 +/- 70.25\n",
      "Eval num_timesteps=200000, episode_reward=6.97 +/- 20.95\n",
      "Episode length: 14.71 +/- 18.83\n",
      "  Mean reward (shaped): 8.75\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 18/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-199.93 +/- 0.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-185.06 +/- 53.93\n",
      "Episode length: 186.71 +/- 48.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-199.69 +/- 0.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-91.70 +/- 103.91\n",
      "Episode length: 103.20 +/- 93.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-37.35 +/- 86.24\n",
      "Episode length: 54.54 +/- 77.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-41.27 +/- 89.04\n",
      "Episode length: 58.00 +/- 79.83\n",
      "Eval num_timesteps=140000, episode_reward=-28.18 +/- 80.43\n",
      "Episode length: 46.18 +/- 72.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-22.27 +/- 74.61\n",
      "Episode length: 40.95 +/- 66.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-12.04 +/- 62.58\n",
      "Episode length: 31.75 +/- 56.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-3.72 +/- 49.62\n",
      "Episode length: 24.29 +/- 44.45\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 1.77\n",
      "  Success rate (default): 96.50%\n",
      "\n",
      "Configuration 19/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-172.35 +/- 70.74\n",
      "Episode length: 175.47 +/- 63.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-9.83 +/- 59.89\n",
      "Episode length: 29.76 +/- 53.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-41.31 +/- 88.91\n",
      "Episode length: 58.06 +/- 79.80\n",
      "Eval num_timesteps=80000, episode_reward=4.47 +/- 29.30\n",
      "Episode length: 16.97 +/- 26.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=9.01 +/- 2.39\n",
      "Episode length: 12.86 +/- 2.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=6.51 +/- 20.91\n",
      "Episode length: 15.18 +/- 18.77\n",
      "Eval num_timesteps=140000, episode_reward=8.96 +/- 2.33\n",
      "Episode length: 12.95 +/- 2.46\n",
      "Eval num_timesteps=160000, episode_reward=4.51 +/- 29.31\n",
      "Episode length: 16.96 +/- 26.26\n",
      "Eval num_timesteps=180000, episode_reward=4.38 +/- 29.31\n",
      "Episode length: 17.09 +/- 26.27\n",
      "Eval num_timesteps=200000, episode_reward=4.82 +/- 29.38\n",
      "Episode length: 16.62 +/- 26.33\n",
      "  Mean reward (shaped): 4.81\n",
      "  Success rate (default): 98.50%\n",
      "\n",
      "Configuration 20/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-199.95 +/- 0.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-149.52 +/- 89.44\n",
      "Episode length: 154.95 +/- 80.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-55.66 +/- 94.42\n",
      "Episode length: 71.06 +/- 84.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-99.51 +/- 104.15\n",
      "Episode length: 110.22 +/- 93.46\n",
      "Eval num_timesteps=100000, episode_reward=-4.05 +/- 49.57\n",
      "Episode length: 24.70 +/- 44.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-107.66 +/- 103.73\n",
      "Episode length: 117.52 +/- 93.06\n",
      "Eval num_timesteps=140000, episode_reward=-51.31 +/- 94.64\n",
      "Episode length: 67.04 +/- 85.01\n",
      "Eval num_timesteps=160000, episode_reward=2.74 +/- 35.73\n",
      "Episode length: 18.51 +/- 32.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-155.85 +/- 84.98\n",
      "Episode length: 160.75 +/- 76.14\n",
      "Eval num_timesteps=200000, episode_reward=-84.95 +/- 103.73\n",
      "Episode length: 97.22 +/- 92.99\n",
      "  Mean reward (shaped): -78.64\n",
      "  Success rate (default): 59.50%\n",
      "\n",
      "Saved PPO tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_tuning_distance_based.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_distance_based_best.yaml\n",
      "\n",
      "======================================================================\n",
      "PPO tuning for reward: modified_penalty\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-91.97 +/- 29.25\n",
      "Episode length: 186.82 +/- 48.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-88.45 +/- 34.64\n",
      "Episode length: 181.01 +/- 56.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-56.48 +/- 55.60\n",
      "Episode length: 128.53 +/- 91.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-55.44 +/- 55.74\n",
      "Episode length: 126.86 +/- 91.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-25.75 +/- 54.49\n",
      "Episode length: 78.16 +/- 89.43\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -5.46\n",
      "  Success rate (default): 84.00%\n",
      "\n",
      "Configuration 2/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-74.83 +/- 47.39\n",
      "Episode length: 158.69 +/- 77.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-62.24 +/- 53.80\n",
      "Episode length: 138.02 +/- 88.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=11.54 +/- 15.98\n",
      "Episode length: 17.11 +/- 26.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=12.86 +/- 11.42\n",
      "Episode length: 14.88 +/- 18.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=14.06 +/- 1.34\n",
      "Episode length: 12.87 +/- 2.69\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-9.95 +/- 46.44\n",
      "Episode length: 52.29 +/- 76.20\n",
      "Eval num_timesteps=140000, episode_reward=-29.30 +/- 55.36\n",
      "Episode length: 84.02 +/- 90.81\n",
      "Eval num_timesteps=160000, episode_reward=-19.93 +/- 52.43\n",
      "Episode length: 68.56 +/- 86.08\n",
      "Eval num_timesteps=180000, episode_reward=-19.14 +/- 51.69\n",
      "Episode length: 67.40 +/- 84.77\n",
      "Eval num_timesteps=200000, episode_reward=12.97 +/- 11.43\n",
      "Episode length: 14.65 +/- 18.81\n",
      "  Mean reward (shaped): 12.83\n",
      "  Success rate (default): 96.00%\n",
      "\n",
      "Configuration 3/20: {'learning_rate': 0.0003, 'n_steps': 1024, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-87.30 +/- 36.14\n",
      "Episode length: 179.10 +/- 59.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-57.78 +/- 55.09\n",
      "Episode length: 130.74 +/- 90.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-55.30 +/- 55.91\n",
      "Episode length: 126.59 +/- 91.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-65.91 +/- 52.08\n",
      "Episode length: 144.12 +/- 85.37\n",
      "Eval num_timesteps=120000, episode_reward=-44.15 +/- 56.99\n",
      "Episode length: 108.39 +/- 93.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-30.57 +/- 55.53\n",
      "Episode length: 86.14 +/- 91.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-17.93 +/- 51.19\n",
      "Episode length: 65.38 +/- 83.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-14.82 +/- 49.19\n",
      "Episode length: 60.39 +/- 80.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-18.09 +/- 51.10\n",
      "Episode length: 65.69 +/- 83.79\n",
      "  Mean reward (shaped): -13.43\n",
      "  Success rate (default): 70.00%\n",
      "\n",
      "Configuration 4/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-97.67 +/- 16.35\n",
      "Episode length: 196.15 +/- 26.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-97.67 +/- 16.35\n",
      "Episode length: 196.15 +/- 26.95\n",
      "Eval num_timesteps=140000, episode_reward=-98.84 +/- 11.54\n",
      "Episode length: 198.09 +/- 19.00\n",
      "Eval num_timesteps=160000, episode_reward=-88.60 +/- 34.21\n",
      "Episode length: 181.30 +/- 56.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-86.40 +/- 36.83\n",
      "Episode length: 177.72 +/- 60.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-86.42 +/- 36.80\n",
      "Episode length: 177.75 +/- 60.27\n",
      "  Mean reward (shaped): -81.95\n",
      "  Success rate (default): 14.50%\n",
      "\n",
      "Configuration 5/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-53.77 +/- 55.47\n",
      "Episode length: 124.34 +/- 90.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-7.76 +/- 44.69\n",
      "Episode length: 48.73 +/- 73.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-17.93 +/- 51.20\n",
      "Episode length: 65.37 +/- 83.99\n",
      "Eval num_timesteps=100000, episode_reward=14.12 +/- 1.32\n",
      "Episode length: 12.75 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=8.48 +/- 24.92\n",
      "Episode length: 21.99 +/- 40.91\n",
      "Eval num_timesteps=140000, episode_reward=10.76 +/- 19.51\n",
      "Episode length: 18.25 +/- 32.05\n",
      "Eval num_timesteps=160000, episode_reward=13.90 +/- 1.33\n",
      "Episode length: 13.20 +/- 2.65\n",
      "Eval num_timesteps=180000, episode_reward=14.11 +/- 1.34\n",
      "Episode length: 12.79 +/- 2.69\n",
      "Eval num_timesteps=200000, episode_reward=9.43 +/- 22.38\n",
      "Episode length: 20.50 +/- 36.74\n",
      "  Mean reward (shaped): 6.09\n",
      "  Success rate (default): 94.00%\n",
      "\n",
      "Configuration 6/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-77.22 +/- 45.57\n",
      "Episode length: 162.64 +/- 74.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-39.47 +/- 57.01\n",
      "Episode length: 100.67 +/- 93.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=13.87 +/- 1.63\n",
      "Episode length: 13.26 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-18.15 +/- 51.06\n",
      "Episode length: 65.82 +/- 83.71\n",
      "Eval num_timesteps=120000, episode_reward=10.40 +/- 19.45\n",
      "Episode length: 18.96 +/- 31.92\n",
      "Eval num_timesteps=140000, episode_reward=0.07 +/- 36.98\n",
      "Episode length: 35.94 +/- 60.64\n",
      "Eval num_timesteps=160000, episode_reward=-46.50 +/- 56.82\n",
      "Episode length: 112.27 +/- 93.17\n",
      "Eval num_timesteps=180000, episode_reward=-13.34 +/- 48.72\n",
      "Episode length: 57.83 +/- 79.93\n",
      "Eval num_timesteps=200000, episode_reward=14.19 +/- 1.18\n",
      "Episode length: 12.63 +/- 2.36\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -42.31\n",
      "  Success rate (default): 53.00%\n",
      "\n",
      "Configuration 7/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-98.83 +/- 11.64\n",
      "Episode length: 198.07 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-97.66 +/- 16.38\n",
      "Episode length: 196.14 +/- 27.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-98.83 +/- 11.64\n",
      "Episode length: 198.07 +/- 19.20\n",
      "Eval num_timesteps=80000, episode_reward=-88.42 +/- 34.74\n",
      "Episode length: 180.94 +/- 57.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-57.65 +/- 55.27\n",
      "Episode length: 130.47 +/- 90.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-55.52 +/- 55.63\n",
      "Episode length: 127.04 +/- 91.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-11.10 +/- 47.23\n",
      "Episode length: 54.18 +/- 77.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-5.42 +/- 42.82\n",
      "Episode length: 44.86 +/- 70.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=0.15 +/- 37.00\n",
      "Episode length: 35.77 +/- 60.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=12.51 +/- 11.37\n",
      "Episode length: 15.58 +/- 18.69\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 12.72\n",
      "  Success rate (default): 99.00%\n",
      "\n",
      "Configuration 8/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-91.94 +/- 29.38\n",
      "Episode length: 186.75 +/- 48.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-81.61 +/- 42.14\n",
      "Episode length: 169.78 +/- 69.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-58.64 +/- 55.15\n",
      "Episode length: 132.04 +/- 90.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-52.08 +/- 56.32\n",
      "Episode length: 121.37 +/- 92.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-47.70 +/- 56.67\n",
      "Episode length: 114.27 +/- 92.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=2.62 +/- 34.23\n",
      "Episode length: 31.65 +/- 56.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=6.93 +/- 27.04\n",
      "Episode length: 24.67 +/- 44.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=12.62 +/- 11.39\n",
      "Episode length: 15.36 +/- 18.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=13.88 +/- 1.21\n",
      "Episode length: 13.25 +/- 2.42\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 13.98\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 9/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-90.88 +/- 30.93\n",
      "Episode length: 185.04 +/- 50.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=7.20 +/- 27.11\n",
      "Episode length: 24.15 +/- 44.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=10.40 +/- 19.46\n",
      "Episode length: 18.97 +/- 31.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-15.69 +/- 49.99\n",
      "Episode length: 61.72 +/- 82.00\n",
      "Eval num_timesteps=120000, episode_reward=8.30 +/- 24.88\n",
      "Episode length: 22.35 +/- 40.84\n",
      "Eval num_timesteps=140000, episode_reward=12.78 +/- 11.42\n",
      "Episode length: 15.04 +/- 18.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-48.56 +/- 56.88\n",
      "Episode length: 115.57 +/- 93.36\n",
      "Eval num_timesteps=180000, episode_reward=-15.72 +/- 49.96\n",
      "Episode length: 61.79 +/- 81.95\n",
      "Eval num_timesteps=200000, episode_reward=4.74 +/- 30.91\n",
      "Episode length: 28.25 +/- 50.71\n",
      "  Mean reward (shaped): 14.04\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 10/20: {'learning_rate': 0.0003, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-98.84 +/- 11.54\n",
      "Episode length: 198.09 +/- 19.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-80.49 +/- 43.11\n",
      "Episode length: 167.95 +/- 70.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-88.48 +/- 34.55\n",
      "Episode length: 181.07 +/- 56.79\n",
      "Eval num_timesteps=120000, episode_reward=-79.50 +/- 43.77\n",
      "Episode length: 166.37 +/- 71.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-78.47 +/- 44.46\n",
      "Episode length: 164.74 +/- 72.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-74.90 +/- 47.27\n",
      "Episode length: 158.82 +/- 77.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-61.17 +/- 54.11\n",
      "Episode length: 136.28 +/- 88.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-97.68 +/- 16.24\n",
      "Episode length: 196.18 +/- 26.74\n",
      "  Mean reward (shaped): -89.01\n",
      "  Success rate (default): 13.00%\n",
      "\n",
      "Configuration 11/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-66.92 +/- 51.77\n",
      "Episode length: 145.72 +/- 84.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-2.98 +/- 40.77\n",
      "Episode length: 40.82 +/- 66.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=13.92 +/- 1.35\n",
      "Episode length: 13.16 +/- 2.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-46.60 +/- 56.71\n",
      "Episode length: 112.47 +/- 92.96\n",
      "Eval num_timesteps=120000, episode_reward=13.91 +/- 1.17\n",
      "Episode length: 13.18 +/- 2.34\n",
      "Eval num_timesteps=140000, episode_reward=13.74 +/- 1.22\n",
      "Episode length: 13.52 +/- 2.44\n",
      "Eval num_timesteps=160000, episode_reward=13.99 +/- 1.25\n",
      "Episode length: 13.02 +/- 2.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=13.98 +/- 1.18\n",
      "Episode length: 13.03 +/- 2.36\n",
      "Eval num_timesteps=200000, episode_reward=14.02 +/- 1.19\n",
      "Episode length: 12.97 +/- 2.38\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 14.07\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 12/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-81.77 +/- 41.79\n",
      "Episode length: 170.09 +/- 68.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-95.36 +/- 22.76\n",
      "Episode length: 192.35 +/- 37.48\n",
      "Eval num_timesteps=180000, episode_reward=-86.41 +/- 36.82\n",
      "Episode length: 177.73 +/- 60.31\n",
      "Eval num_timesteps=200000, episode_reward=-88.66 +/- 34.04\n",
      "Episode length: 181.41 +/- 55.78\n",
      "  Mean reward (shaped): -86.50\n",
      "  Success rate (default): 14.50%\n",
      "\n",
      "Configuration 13/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-98.84 +/- 11.54\n",
      "Episode length: 198.09 +/- 19.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-89.61 +/- 33.05\n",
      "Episode length: 182.90 +/- 54.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-60.09 +/- 54.39\n",
      "Episode length: 134.53 +/- 89.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=1.69 +/- 35.77\n",
      "Episode length: 33.11 +/- 58.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=7.32 +/- 27.14\n",
      "Episode length: 23.91 +/- 44.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-60.09 +/- 54.40\n",
      "Episode length: 134.52 +/- 89.25\n",
      "Eval num_timesteps=160000, episode_reward=-34.90 +/- 56.55\n",
      "Episode length: 93.17 +/- 92.82\n",
      "Eval num_timesteps=180000, episode_reward=13.74 +/- 1.36\n",
      "Episode length: 13.51 +/- 2.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=12.78 +/- 11.41\n",
      "Episode length: 15.04 +/- 18.76\n",
      "  Mean reward (shaped): -0.85\n",
      "  Success rate (default): 91.00%\n",
      "\n",
      "Configuration 14/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.3, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-90.76 +/- 31.34\n",
      "Episode length: 184.80 +/- 51.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-67.23 +/- 51.29\n",
      "Episode length: 146.35 +/- 83.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-79.52 +/- 43.72\n",
      "Episode length: 166.42 +/- 71.70\n",
      "Eval num_timesteps=180000, episode_reward=-90.78 +/- 31.28\n",
      "Episode length: 184.83 +/- 51.45\n",
      "Eval num_timesteps=200000, episode_reward=-89.61 +/- 33.04\n",
      "Episode length: 182.91 +/- 54.34\n",
      "  Mean reward (shaped): -87.90\n",
      "  Success rate (default): 6.50%\n",
      "\n",
      "Configuration 15/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-78.09 +/- 45.24\n",
      "Episode length: 163.97 +/- 74.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-21.26 +/- 52.79\n",
      "Episode length: 70.81 +/- 86.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=13.95 +/- 1.26\n",
      "Episode length: 13.11 +/- 2.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=12.79 +/- 11.42\n",
      "Episode length: 15.01 +/- 18.79\n",
      "Eval num_timesteps=120000, episode_reward=10.81 +/- 19.53\n",
      "Episode length: 18.14 +/- 32.09\n",
      "Eval num_timesteps=140000, episode_reward=13.89 +/- 1.22\n",
      "Episode length: 13.22 +/- 2.45\n",
      "Eval num_timesteps=160000, episode_reward=10.52 +/- 19.48\n",
      "Episode length: 18.74 +/- 31.99\n",
      "Eval num_timesteps=180000, episode_reward=14.05 +/- 1.38\n",
      "Episode length: 12.89 +/- 2.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=13.89 +/- 1.38\n",
      "Episode length: 13.21 +/- 2.77\n",
      "  Mean reward (shaped): 13.90\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 16/20: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-86.36 +/- 36.94\n",
      "Episode length: 177.64 +/- 60.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-7.68 +/- 44.72\n",
      "Episode length: 48.58 +/- 73.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=13.90 +/- 1.39\n",
      "Episode length: 13.19 +/- 2.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-8.60 +/- 45.72\n",
      "Episode length: 49.99 +/- 75.04\n",
      "Eval num_timesteps=160000, episode_reward=12.67 +/- 11.39\n",
      "Episode length: 15.25 +/- 18.73\n",
      "Eval num_timesteps=180000, episode_reward=14.04 +/- 1.38\n",
      "Episode length: 12.92 +/- 2.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=13.80 +/- 1.27\n",
      "Episode length: 13.39 +/- 2.55\n",
      "  Mean reward (shaped): 14.08\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 17/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-90.89 +/- 30.91\n",
      "Episode length: 185.05 +/- 50.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-63.33 +/- 53.47\n",
      "Episode length: 139.77 +/- 87.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-54.33 +/- 55.95\n",
      "Episode length: 125.05 +/- 91.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-28.49 +/- 54.81\n",
      "Episode length: 82.81 +/- 89.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-18.09 +/- 51.09\n",
      "Episode length: 65.71 +/- 83.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-14.75 +/- 49.24\n",
      "Episode length: 60.25 +/- 80.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-23.91 +/- 53.42\n",
      "Episode length: 75.28 +/- 87.56\n",
      "  Mean reward (shaped): -11.81\n",
      "  Success rate (default): 74.00%\n",
      "\n",
      "Configuration 18/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-78.27 +/- 44.87\n",
      "Episode length: 164.33 +/- 73.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-55.33 +/- 55.87\n",
      "Episode length: 126.65 +/- 91.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=9.43 +/- 22.37\n",
      "Episode length: 20.50 +/- 36.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=13.96 +/- 1.33\n",
      "Episode length: 13.07 +/- 2.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=14.24 +/- 1.25\n",
      "Episode length: 12.51 +/- 2.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-9.89 +/- 46.47\n",
      "Episode length: 52.18 +/- 76.25\n",
      "Eval num_timesteps=140000, episode_reward=-41.81 +/- 57.04\n",
      "Episode length: 104.53 +/- 93.59\n",
      "Eval num_timesteps=160000, episode_reward=-28.07 +/- 55.13\n",
      "Episode length: 81.97 +/- 90.47\n",
      "Eval num_timesteps=180000, episode_reward=-23.55 +/- 53.66\n",
      "Episode length: 74.58 +/- 88.05\n",
      "Eval num_timesteps=200000, episode_reward=13.85 +/- 1.15\n",
      "Episode length: 13.31 +/- 2.29\n",
      "  Mean reward (shaped): 12.35\n",
      "  Success rate (default): 98.50%\n",
      "\n",
      "Configuration 19/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 128, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-96.59 +/- 19.36\n",
      "Episode length: 194.42 +/- 31.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-89.83 +/- 32.32\n",
      "Episode length: 183.36 +/- 52.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-48.70 +/- 56.73\n",
      "Episode length: 115.84 +/- 93.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-69.33 +/- 50.44\n",
      "Episode length: 149.73 +/- 82.68\n",
      "Eval num_timesteps=120000, episode_reward=10.31 +/- 19.45\n",
      "Episode length: 19.15 +/- 31.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=13.87 +/- 1.23\n",
      "Episode length: 13.27 +/- 2.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=13.79 +/- 1.41\n",
      "Episode length: 13.43 +/- 2.83\n",
      "Eval num_timesteps=180000, episode_reward=-13.53 +/- 48.61\n",
      "Episode length: 58.21 +/- 79.71\n",
      "Eval num_timesteps=200000, episode_reward=14.07 +/- 1.23\n",
      "Episode length: 12.85 +/- 2.46\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): 13.75\n",
      "  Success rate (default): 100.00%\n",
      "\n",
      "Configuration 20/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-51.43 +/- 55.93\n",
      "Episode length: 120.49 +/- 91.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=1.33 +/- 35.65\n",
      "Episode length: 33.82 +/- 58.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-17.98 +/- 51.16\n",
      "Episode length: 65.47 +/- 83.92\n",
      "Eval num_timesteps=100000, episode_reward=13.82 +/- 1.29\n",
      "Episode length: 13.36 +/- 2.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=0.42 +/- 37.10\n",
      "Episode length: 35.24 +/- 60.90\n",
      "Eval num_timesteps=140000, episode_reward=8.11 +/- 24.83\n",
      "Episode length: 22.73 +/- 40.75\n",
      "Eval num_timesteps=160000, episode_reward=14.06 +/- 1.42\n",
      "Episode length: 12.87 +/- 2.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=13.91 +/- 1.23\n",
      "Episode length: 13.18 +/- 2.46\n",
      "Eval num_timesteps=200000, episode_reward=6.00 +/- 29.11\n",
      "Episode length: 26.14 +/- 47.76\n",
      "  Mean reward (shaped): 7.63\n",
      "  Success rate (default): 93.50%\n",
      "\n",
      "Saved PPO tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_tuning_modified_penalty.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_modified_penalty_best.yaml\n",
      "\n",
      "======================================================================\n",
      "PPO tuning for reward: enhanced\n",
      "======================================================================\n",
      "\n",
      "Configuration 1/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-142.37 +/- 90.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-110.92 +/- 98.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-112.82 +/- 98.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-98.83 +/- 99.13\n",
      "Episode length: 198.11 +/- 18.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-75.51 +/- 95.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-67.86 +/- 92.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-56.10 +/- 87.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-77.80 +/- 95.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-108.84 +/- 98.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-63.67 +/- 91.31\n",
      "Episode length: 196.28 +/- 26.04\n",
      "  Mean reward (shaped): -58.91\n",
      "  Success rate (default): 5.50%\n",
      "\n",
      "Configuration 2/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-108.14 +/- 99.15\n",
      "Episode length: 192.46 +/- 36.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-89.99 +/- 99.35\n",
      "Episode length: 183.21 +/- 53.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-47.30 +/- 86.05\n",
      "Episode length: 164.35 +/- 73.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-32.76 +/- 73.15\n",
      "Episode length: 183.08 +/- 53.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-7.32 +/- 40.56\n",
      "Episode length: 153.26 +/- 80.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-40.24 +/- 82.66\n",
      "Episode length: 162.51 +/- 74.99\n",
      "Eval num_timesteps=140000, episode_reward=-29.71 +/- 72.02\n",
      "Episode length: 162.60 +/- 74.81\n",
      "Eval num_timesteps=160000, episode_reward=-79.79 +/- 100.33\n",
      "Episode length: 164.31 +/- 73.71\n",
      "Eval num_timesteps=180000, episode_reward=-54.85 +/- 90.86\n",
      "Episode length: 162.63 +/- 74.76\n",
      "Eval num_timesteps=200000, episode_reward=-23.74 +/- 65.51\n",
      "Episode length: 173.54 +/- 65.59\n",
      "  Mean reward (shaped): -26.84\n",
      "  Success rate (default): 10.00%\n",
      "\n",
      "Configuration 3/20: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-126.62 +/- 95.61\n",
      "Episode length: 196.17 +/- 26.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-114.80 +/- 98.05\n",
      "Episode length: 183.18 +/- 53.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-79.09 +/- 98.05\n",
      "Episode length: 166.46 +/- 71.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-103.19 +/- 102.57\n",
      "Episode length: 162.43 +/- 75.16\n",
      "Eval num_timesteps=100000, episode_reward=-31.13 +/- 77.00\n",
      "Episode length: 147.45 +/- 84.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-73.69 +/- 94.53\n",
      "Episode length: 198.09 +/- 19.00\n",
      "Eval num_timesteps=140000, episode_reward=-44.73 +/- 87.59\n",
      "Episode length: 152.89 +/- 81.61\n",
      "Eval num_timesteps=160000, episode_reward=-98.94 +/- 102.09\n",
      "Episode length: 164.31 +/- 73.70\n",
      "Eval num_timesteps=180000, episode_reward=-99.01 +/- 100.92\n",
      "Episode length: 179.23 +/- 59.08\n",
      "Eval num_timesteps=200000, episode_reward=-9.76 +/- 63.37\n",
      "Episode length: 81.94 +/- 90.50\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -6.98\n",
      "  Success rate (default): 47.50%\n",
      "\n",
      "Configuration 4/20: {'learning_rate': 0.0005, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-107.35 +/- 98.28\n",
      "Episode length: 198.07 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-83.94 +/- 96.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-110.54 +/- 98.79\n",
      "Episode length: 188.59 +/- 45.17\n",
      "Eval num_timesteps=80000, episode_reward=-103.32 +/- 98.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-122.64 +/- 96.57\n",
      "Episode length: 190.76 +/- 40.28\n",
      "Eval num_timesteps=120000, episode_reward=-80.33 +/- 97.65\n",
      "Episode length: 186.80 +/- 48.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-151.58 +/- 83.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-87.33 +/- 99.34\n",
      "Episode length: 183.03 +/- 53.96\n",
      "Eval num_timesteps=180000, episode_reward=-116.47 +/- 96.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-64.48 +/- 90.87\n",
      "Episode length: 183.31 +/- 53.07\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -52.37\n",
      "  Success rate (default): 3.50%\n",
      "\n",
      "Configuration 5/20: {'learning_rate': 0.0001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-148.56 +/- 86.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-57.86 +/- 88.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-79.77 +/- 96.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-132.16 +/- 92.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-78.75 +/- 98.85\n",
      "Episode length: 170.25 +/- 68.19\n",
      "Eval num_timesteps=120000, episode_reward=-55.26 +/- 90.26\n",
      "Episode length: 170.37 +/- 67.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-55.30 +/- 92.15\n",
      "Episode length: 151.92 +/- 81.12\n",
      "Eval num_timesteps=160000, episode_reward=-4.24 +/- 45.99\n",
      "Episode length: 114.23 +/- 92.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-35.49 +/- 82.55\n",
      "Episode length: 127.36 +/- 90.87\n",
      "Eval num_timesteps=200000, episode_reward=5.54 +/- 10.42\n",
      "Episode length: 108.84 +/- 93.02\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -8.41\n",
      "  Success rate (default): 62.00%\n",
      "\n",
      "Configuration 6/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 128, 'clip_range': 0.3, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-114.40 +/- 98.24\n",
      "Episode length: 194.39 +/- 31.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-104.63 +/- 98.86\n",
      "Episode length: 194.41 +/- 31.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-177.08 +/- 65.47\n",
      "Episode length: 181.04 +/- 56.88\n",
      "Eval num_timesteps=80000, episode_reward=-65.89 +/- 95.35\n",
      "Episode length: 160.65 +/- 76.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-137.00 +/- 89.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-39.33 +/- 78.00\n",
      "Episode length: 181.29 +/- 56.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-21.02 +/- 69.77\n",
      "Episode length: 127.11 +/- 91.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-94.02 +/- 101.46\n",
      "Episode length: 166.63 +/- 71.23\n",
      "Eval num_timesteps=180000, episode_reward=-30.77 +/- 79.85\n",
      "Episode length: 121.29 +/- 92.51\n",
      "Eval num_timesteps=200000, episode_reward=-131.55 +/- 97.72\n",
      "Episode length: 158.88 +/- 77.44\n",
      "  Mean reward (shaped): -54.40\n",
      "  Success rate (default): 19.50%\n",
      "\n",
      "Configuration 7/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-138.75 +/- 91.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-125.08 +/- 95.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-115.06 +/- 97.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-112.98 +/- 98.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-70.10 +/- 98.98\n",
      "Episode length: 147.90 +/- 83.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-80.56 +/- 105.93\n",
      "Episode length: 113.77 +/- 93.45\n",
      "Eval num_timesteps=140000, episode_reward=-58.74 +/- 92.43\n",
      "Episode length: 168.11 +/- 70.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-104.67 +/- 97.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-36.66 +/- 87.07\n",
      "Episode length: 110.19 +/- 93.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-129.45 +/- 96.57\n",
      "Episode length: 177.27 +/- 61.56\n",
      "  Mean reward (shaped): -76.70\n",
      "  Success rate (default): 15.00%\n",
      "\n",
      "Configuration 8/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-122.82 +/- 96.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-107.26 +/- 98.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-121.02 +/- 96.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-93.11 +/- 98.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-101.77 +/- 100.01\n",
      "Episode length: 186.85 +/- 47.93\n",
      "Eval num_timesteps=120000, episode_reward=-122.97 +/- 95.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-84.31 +/- 97.71\n",
      "Episode length: 181.35 +/- 55.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-34.20 +/- 75.36\n",
      "Episode length: 172.16 +/- 66.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-30.75 +/- 71.50\n",
      "Episode length: 170.10 +/- 68.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-31.28 +/- 76.81\n",
      "Episode length: 143.84 +/- 85.80\n",
      "  Mean reward (shaped): -26.39\n",
      "  Success rate (default): 36.00%\n",
      "\n",
      "Configuration 9/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-148.37 +/- 86.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-150.41 +/- 85.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-132.27 +/- 93.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-104.60 +/- 99.07\n",
      "Episode length: 194.36 +/- 32.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-55.25 +/- 87.83\n",
      "Episode length: 187.24 +/- 46.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-120.85 +/- 96.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-95.61 +/- 98.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-61.00 +/- 90.85\n",
      "Episode length: 188.74 +/- 44.57\n",
      "Eval num_timesteps=180000, episode_reward=-174.52 +/- 66.78\n",
      "Episode length: 190.54 +/- 41.24\n",
      "Eval num_timesteps=200000, episode_reward=-75.70 +/- 98.94\n",
      "Episode length: 162.69 +/- 74.63\n",
      "  Mean reward (shaped): -73.95\n",
      "  Success rate (default): 14.00%\n",
      "\n",
      "Configuration 10/20: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 32, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-93.69 +/- 106.31\n",
      "Episode length: 133.89 +/- 90.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-43.22 +/- 83.43\n",
      "Episode length: 173.72 +/- 65.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-147.79 +/- 86.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-21.79 +/- 80.03\n",
      "Episode length: 66.95 +/- 85.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-20.52 +/- 78.33\n",
      "Episode length: 64.00 +/- 82.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-69.07 +/- 104.92\n",
      "Episode length: 87.88 +/- 91.57\n",
      "Eval num_timesteps=140000, episode_reward=-1.41 +/- 58.85\n",
      "Episode length: 40.45 +/- 67.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-37.49 +/- 81.81\n",
      "Episode length: 140.62 +/- 86.57\n",
      "Eval num_timesteps=180000, episode_reward=-43.62 +/- 88.21\n",
      "Episode length: 127.32 +/- 90.91\n",
      "Eval num_timesteps=200000, episode_reward=-11.80 +/- 69.17\n",
      "Episode length: 59.94 +/- 80.91\n",
      "  Mean reward (shaped): -71.49\n",
      "  Success rate (default): 55.50%\n",
      "\n",
      "Configuration 11/20: {'learning_rate': 0.0005, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-122.97 +/- 96.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-138.72 +/- 91.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-78.00 +/- 95.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-31.80 +/- 70.87\n",
      "Episode length: 185.17 +/- 50.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-59.95 +/- 89.62\n",
      "Episode length: 188.59 +/- 45.16\n",
      "Eval num_timesteps=120000, episode_reward=-80.04 +/- 97.73\n",
      "Episode length: 183.35 +/- 52.95\n",
      "Eval num_timesteps=140000, episode_reward=-36.24 +/- 76.87\n",
      "Episode length: 177.71 +/- 60.36\n",
      "Eval num_timesteps=160000, episode_reward=-59.11 +/- 90.06\n",
      "Episode length: 188.80 +/- 44.33\n",
      "Eval num_timesteps=180000, episode_reward=-90.13 +/- 100.89\n",
      "Episode length: 168.13 +/- 70.43\n",
      "Eval num_timesteps=200000, episode_reward=-57.23 +/- 89.08\n",
      "Episode length: 181.35 +/- 55.95\n",
      "  Mean reward (shaped): -25.39\n",
      "  Success rate (default): 9.00%\n",
      "\n",
      "Configuration 12/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-164.37 +/- 76.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-170.20 +/- 70.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-172.37 +/- 68.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-132.67 +/- 93.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-132.62 +/- 93.81\n",
      "Episode length: 198.09 +/- 19.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-177.58 +/- 61.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-138.53 +/- 89.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-97.45 +/- 98.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-111.28 +/- 97.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-138.73 +/- 91.25\n",
      "Episode length: 198.09 +/- 19.00\n",
      "  Mean reward (shaped): -113.40\n",
      "  Success rate (default): 1.50%\n",
      "\n",
      "Configuration 13/20: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 64, 'clip_range': 0.2, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-161.99 +/- 78.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-121.23 +/- 96.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-84.02 +/- 96.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-85.99 +/- 96.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-43.58 +/- 80.66\n",
      "Episode length: 190.45 +/- 41.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-70.56 +/- 96.78\n",
      "Episode length: 171.72 +/- 67.32\n",
      "Eval num_timesteps=140000, episode_reward=-72.02 +/- 96.11\n",
      "Episode length: 182.88 +/- 54.44\n",
      "Eval num_timesteps=160000, episode_reward=-33.38 +/- 81.18\n",
      "Episode length: 125.76 +/- 90.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-86.67 +/- 97.15\n",
      "Episode length: 181.53 +/- 55.41\n",
      "Eval num_timesteps=200000, episode_reward=-52.22 +/- 92.47\n",
      "Episode length: 136.53 +/- 88.45\n",
      "  Mean reward (shaped): -21.14\n",
      "  Success rate (default): 37.50%\n",
      "\n",
      "Configuration 14/20: {'learning_rate': 0.0001, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-110.96 +/- 98.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-77.56 +/- 95.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-71.01 +/- 94.61\n",
      "Episode length: 190.56 +/- 41.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-75.05 +/- 95.48\n",
      "Episode length: 194.39 +/- 31.90\n",
      "Eval num_timesteps=100000, episode_reward=-58.36 +/- 90.53\n",
      "Episode length: 183.04 +/- 53.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-37.79 +/- 78.84\n",
      "Episode length: 175.50 +/- 63.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-73.37 +/- 99.26\n",
      "Episode length: 162.24 +/- 75.53\n",
      "Eval num_timesteps=160000, episode_reward=-41.32 +/- 82.20\n",
      "Episode length: 167.74 +/- 71.29\n",
      "Eval num_timesteps=180000, episode_reward=-63.54 +/- 93.79\n",
      "Episode length: 171.82 +/- 67.08\n",
      "Eval num_timesteps=200000, episode_reward=-29.45 +/- 72.07\n",
      "Episode length: 166.34 +/- 71.85\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -51.08\n",
      "  Success rate (default): 19.50%\n",
      "\n",
      "Configuration 15/20: {'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'clip_range': 0.1, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-149.96 +/- 85.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-42.67 +/- 83.44\n",
      "Episode length: 161.25 +/- 75.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=10.87 +/- 30.61\n",
      "Episode length: 28.24 +/- 50.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=16.56 +/- 2.10\n",
      "Episode length: 13.30 +/- 2.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-31.43 +/- 85.64\n",
      "Episode length: 80.60 +/- 89.58\n",
      "Eval num_timesteps=120000, episode_reward=10.11 +/- 37.01\n",
      "Episode length: 18.86 +/- 31.95\n",
      "Eval num_timesteps=140000, episode_reward=16.80 +/- 2.49\n",
      "Episode length: 12.95 +/- 2.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=16.82 +/- 2.44\n",
      "Episode length: 12.97 +/- 2.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-59.01 +/- 101.39\n",
      "Episode length: 91.07 +/- 92.72\n",
      "Eval num_timesteps=200000, episode_reward=11.94 +/- 30.36\n",
      "Episode length: 17.35 +/- 26.22\n",
      "  Mean reward (shaped): 5.04\n",
      "  Success rate (default): 89.00%\n",
      "\n",
      "Configuration 16/20: {'learning_rate': 0.0005, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-143.00 +/- 91.23\n",
      "Episode length: 183.04 +/- 53.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-81.14 +/- 97.16\n",
      "Episode length: 185.15 +/- 50.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-145.98 +/- 87.94\n",
      "Episode length: 196.12 +/- 27.16\n",
      "Eval num_timesteps=80000, episode_reward=-68.33 +/- 94.64\n",
      "Episode length: 181.35 +/- 55.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-124.60 +/- 94.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-65.35 +/- 94.52\n",
      "Episode length: 175.66 +/- 62.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-30.96 +/- 71.36\n",
      "Episode length: 175.49 +/- 63.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-21.89 +/- 60.01\n",
      "Episode length: 169.95 +/- 68.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-44.23 +/- 82.73\n",
      "Episode length: 175.51 +/- 63.36\n",
      "Eval num_timesteps=200000, episode_reward=-14.23 +/- 51.62\n",
      "Episode length: 162.79 +/- 74.44\n",
      "New best mean reward!\n",
      "  Mean reward (shaped): -15.64\n",
      "  Success rate (default): 24.00%\n",
      "\n",
      "Configuration 17/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 128, 'clip_range': 0.2, 'ent_coef': 0.05}\n",
      "Eval num_timesteps=20000, episode_reward=-144.40 +/- 88.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-79.85 +/- 96.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-70.29 +/- 92.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-44.05 +/- 83.17\n",
      "Episode length: 168.32 +/- 70.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-55.54 +/- 87.99\n",
      "Episode length: 179.52 +/- 58.26\n",
      "Eval num_timesteps=120000, episode_reward=-18.05 +/- 57.96\n",
      "Episode length: 160.79 +/- 76.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-42.23 +/- 78.86\n",
      "Episode length: 194.30 +/- 32.41\n",
      "Eval num_timesteps=160000, episode_reward=-5.24 +/- 29.26\n",
      "Episode length: 171.69 +/- 67.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-23.83 +/- 62.38\n",
      "Episode length: 181.14 +/- 56.59\n",
      "Eval num_timesteps=200000, episode_reward=-31.04 +/- 71.33\n",
      "Episode length: 177.47 +/- 61.02\n",
      "  Mean reward (shaped): -29.16\n",
      "  Success rate (default): 15.50%\n",
      "\n",
      "Configuration 18/20: {'learning_rate': 0.0005, 'n_steps': 512, 'batch_size': 128, 'clip_range': 0.1, 'ent_coef': 0.0}\n",
      "Eval num_timesteps=20000, episode_reward=-93.20 +/- 98.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-173.62 +/- 66.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-83.62 +/- 99.07\n",
      "Episode length: 175.62 +/- 63.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-81.68 +/- 96.48\n",
      "Episode length: 190.77 +/- 40.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-55.37 +/- 87.71\n",
      "Episode length: 183.30 +/- 53.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-77.05 +/- 100.51\n",
      "Episode length: 158.46 +/- 78.23\n",
      "Eval num_timesteps=140000, episode_reward=-106.82 +/- 99.84\n",
      "Episode length: 179.43 +/- 58.51\n",
      "Eval num_timesteps=160000, episode_reward=-49.20 +/- 84.80\n",
      "Episode length: 190.61 +/- 40.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-48.70 +/- 85.18\n",
      "Episode length: 177.60 +/- 60.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-70.95 +/- 94.12\n",
      "Episode length: 188.78 +/- 44.41\n",
      "  Mean reward (shaped): -60.39\n",
      "  Success rate (default): 17.00%\n",
      "\n",
      "Configuration 19/20: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.3, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-126.59 +/- 95.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-89.62 +/- 97.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-68.06 +/- 92.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-51.04 +/- 86.07\n",
      "Episode length: 182.96 +/- 54.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-57.59 +/- 91.28\n",
      "Episode length: 173.67 +/- 65.27\n",
      "Eval num_timesteps=120000, episode_reward=-53.86 +/- 88.33\n",
      "Episode length: 179.50 +/- 58.32\n",
      "Eval num_timesteps=140000, episode_reward=-16.62 +/- 54.40\n",
      "Episode length: 174.20 +/- 63.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-55.21 +/- 90.47\n",
      "Episode length: 166.71 +/- 71.06\n",
      "Eval num_timesteps=180000, episode_reward=-45.55 +/- 84.64\n",
      "Episode length: 166.97 +/- 70.50\n",
      "Eval num_timesteps=200000, episode_reward=-32.98 +/- 75.99\n",
      "Episode length: 164.35 +/- 73.62\n",
      "  Mean reward (shaped): -14.48\n",
      "  Success rate (default): 30.00%\n",
      "\n",
      "Configuration 20/20: {'learning_rate': 0.001, 'n_steps': 512, 'batch_size': 32, 'clip_range': 0.1, 'ent_coef': 0.01}\n",
      "Eval num_timesteps=20000, episode_reward=-128.63 +/- 94.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-99.61 +/- 98.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-117.04 +/- 97.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-107.23 +/- 98.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-111.65 +/- 99.53\n",
      "Episode length: 177.61 +/- 60.64\n",
      "Eval num_timesteps=120000, episode_reward=-50.62 +/- 89.01\n",
      "Episode length: 148.10 +/- 83.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-101.70 +/- 99.26\n",
      "Episode length: 188.91 +/- 43.90\n",
      "Eval num_timesteps=160000, episode_reward=-30.61 +/- 77.09\n",
      "Episode length: 143.96 +/- 85.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-77.44 +/- 97.93\n",
      "Episode length: 175.62 +/- 63.08\n",
      "Eval num_timesteps=200000, episode_reward=-36.19 +/- 76.96\n",
      "Episode length: 181.26 +/- 56.22\n",
      "  Mean reward (shaped): -24.13\n",
      "  Success rate (default): 0.00%\n",
      "\n",
      "Saved PPO tuning results to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_tuning_enhanced.csv\n",
      "Optimized config saved to: /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_enhanced_best.yaml\n",
      "\n",
      "Saved PPO tuning summary to /Users/azernaczuk/Documents/0sobiste/github/reinforcement_learning_taxi/results/hyperparameter_tuning/ppo_per_reward/ppo_tuning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "summary_rows = []\n",
    "\n",
    "for reward_index, reward_name in enumerate(reward_functions):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f'PPO tuning for reward: {reward_name}')\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    configs = random_sample_params(ppo_param_space, N_RANDOM_CONFIGS, seed=SEED + reward_index * 100)\n",
    "    reward_results = []\n",
    "\n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f'\\nConfiguration {i}/{N_RANDOM_CONFIGS}: {config}')\n",
    "        start_time = time.time()\n",
    "        env = None\n",
    "        eval_env_default = None\n",
    "\n",
    "        try:\n",
    "            env = make_taxi_env(\n",
    "                use_feature_wrapper=True,\n",
    "                reward_wrapper_name=reward_name,\n",
    "                use_action_masking=True,\n",
    "            )\n",
    "\n",
    "            agent = PPOAgent(\n",
    "                env=env,\n",
    "                policy=PPO_BASE_AGENT['policy'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                n_steps=config['n_steps'],\n",
    "                batch_size=config['batch_size'],\n",
    "                n_epochs=PPO_BASE_AGENT['n_epochs'],\n",
    "                gamma=PPO_BASE_AGENT['gamma'],\n",
    "                gae_lambda=PPO_BASE_AGENT['gae_lambda'],\n",
    "                clip_range=config['clip_range'],\n",
    "                clip_range_vf=PPO_BASE_AGENT['clip_range_vf'],\n",
    "                ent_coef=config['ent_coef'],\n",
    "                vf_coef=PPO_BASE_AGENT['vf_coef'],\n",
    "                max_grad_norm=PPO_BASE_AGENT['max_grad_norm'],\n",
    "                policy_kwargs=PPO_BASE_AGENT.get('policy_kwargs') or {},\n",
    "                verbose=0,\n",
    "                seed=PPO_BASE_AGENT.get('seed'),\n",
    "                use_action_masking=True,\n",
    "            )\n",
    "\n",
    "            trainer = PPOTrainer(\n",
    "                env=env,\n",
    "                agent=agent,\n",
    "                log_dir=ROOT_DIR / f'results/logs/reward_tuning/ppo_{reward_name}/config_{i}',\n",
    "                eval_freq=20000,\n",
    "            )\n",
    "            stats = trainer.train(total_timesteps=TRAINING_TIMESTEPS)\n",
    "\n",
    "            eval_shaped = evaluate_agent(agent, env, n_episodes=N_EVAL_EPISODES, deterministic=True)\n",
    "\n",
    "            eval_env_default = make_taxi_env(\n",
    "                use_feature_wrapper=True,\n",
    "                reward_wrapper_name=None,\n",
    "                use_action_masking=True,\n",
    "            )\n",
    "            eval_default = evaluate_agent(\n",
    "                agent,\n",
    "                eval_env_default,\n",
    "                n_episodes=N_EVAL_EPISODES,\n",
    "                deterministic=True,\n",
    "            )\n",
    "\n",
    "            training_time = stats.get('training_time', time.time() - start_time)\n",
    "\n",
    "            reward_results.append({\n",
    "                'config_id': i,\n",
    "                'reward_name': reward_name,\n",
    "                **config,\n",
    "                'mean_reward_shaped': eval_shaped['mean_reward'],\n",
    "                'std_reward_shaped': eval_shaped['std_reward'],\n",
    "                'success_rate_default': eval_default['success_rate'],\n",
    "                'mean_length_default': eval_default['mean_length'],\n",
    "                'training_time': training_time,\n",
    "            })\n",
    "\n",
    "            print(f\"  Mean reward (shaped): {eval_shaped['mean_reward']:.2f}\")\n",
    "            print(f\"  Success rate (default): {eval_default['success_rate']:.2%}\")\n",
    "        except Exception as exc:\n",
    "            print(f'  ERROR: {exc}')\n",
    "        finally:\n",
    "            if eval_env_default is not None:\n",
    "                eval_env_default.close()\n",
    "            if env is not None:\n",
    "                env.close()\n",
    "\n",
    "    reward_df = pd.DataFrame(reward_results)\n",
    "    reward_df.sort_values(by='mean_reward_shaped', ascending=False, inplace=True)\n",
    "\n",
    "    reward_csv = RESULTS_DIR / f'ppo_tuning_{reward_name}.csv'\n",
    "    reward_df.to_csv(reward_csv, index=False)\n",
    "    print(f'\\nSaved PPO tuning results to {reward_csv}')\n",
    "\n",
    "    if not reward_df.empty:\n",
    "        best_row = reward_df.iloc[0]\n",
    "        best_params = {k: best_row[k] for k in ppo_param_space.keys()}\n",
    "        best_yaml = RESULTS_DIR / f'ppo_{reward_name}_best.yaml'\n",
    "        save_optimized_config(\n",
    "            best_params=best_params,\n",
    "            base_config_path=ROOT_DIR / 'src/reinforcement_learning_taxi/configs/ppo_config_baseline.yaml',\n",
    "            output_path=best_yaml,\n",
    "            algorithm='PPO',\n",
    "        )\n",
    "        summary_rows.append({\n",
    "            'reward_name': reward_name,\n",
    "            'best_config_id': int(best_row['config_id']),\n",
    "            'best_mean_reward_shaped': float(best_row['mean_reward_shaped']),\n",
    "            'best_success_rate_default': float(best_row['success_rate_default']),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv = RESULTS_DIR / 'ppo_tuning_summary.csv'\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f'\\nSaved PPO tuning summary to {summary_csv}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
